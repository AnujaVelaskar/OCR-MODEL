{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tkinter import filedialog\n",
    "from tkinter.filedialog import askopenfilename\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "import imutils\n",
    "import os.path\n",
    "import importlib\n",
    "from os import listdir\n",
    "from keras import backend as k\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from playsound import playsound\n",
    "import IPython\n",
    "from gtts import gTTS\n",
    "import pygame\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I entered train\n"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "print (\"I entered train\")\n",
    "# to get the name of the folder\n",
    "for name_folder in os.listdir(\"C:\\\\Users\\HP\\Desktop\\optical-character-recognition-OCR-master\\Fnt\") :\n",
    "    name = 'C:\\\\Users\\HP\\Desktop\\optical-character-recognition-OCR-master\\Fnt/' + name_folder\n",
    "    for f in listdir(name):\n",
    "        # name of the folder is the name of the output\n",
    "        y_train.append(np.asarray(name_folder))\n",
    "y_train = np.asarray(y_train)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving the converted audio in a mp3 file named \n",
    "def image_segmentation(image_name):\n",
    "    #print( \"I entered letter segmentation\")\n",
    "    # reading the image\n",
    "    image = cv2.imread(image_name)\n",
    "\n",
    "    # converting the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # threshold to convert the image to pure black and white\n",
    "    thresh = cv2.threshold(gray, 0,255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "\n",
    "    # find the contours (continous blob of pixels ) in the image \n",
    "    contours = cv2.findContours(thresh,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Hack for compatibility with different OpenCV versions\n",
    "    contours = contours[0] \n",
    "    letter_image_regions = []\n",
    "\n",
    "    # now loop through each of the letter in the image \n",
    "    for contour in contours:\n",
    "        # get the rectangle that contains the contour\n",
    "        x,y,w,h = cv2.boundingRect(contour)\n",
    "        # compare the width and height of the contour to detect if it\n",
    "        # has one letter or not\n",
    "        if w/h >1.25:\n",
    "            # this is too wide for a single letter\n",
    "            continue\n",
    "        elif w<3 or h<3:\n",
    "            # this is a very small image probably a noise\n",
    "            continue\n",
    "        else:\n",
    "        # this is a normal letter by itself\n",
    "            letter_image_regions.append((x,y,w,h))\n",
    "\n",
    "    return letter_image_regions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the trained model\n",
      "loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"loading the trained model\")\n",
    "model = load_model('C:\\\\Users\\HP\\Desktop\\optical-character-recognition-OCR-master\\models/model5.h5')\n",
    "print(\"loaded\")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_for_detection(raw_image):\n",
    "\n",
    "\t#remove tiny noises by blurring\n",
    "\tsm_image = cv2.GaussianBlur(raw_image,(5,5),0)\n",
    "\t\n",
    "\t#binarize\n",
    "\tret, bw_image = cv2.threshold(sm_image,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\t\n",
    "\t#dilate\n",
    "\tkernel = np.ones((2,2),np.uint8)\n",
    "\tbw_image = cv2.dilate(bw_image,kernel)\n",
    "\t\n",
    "\treturn bw_image\n",
    "\t\n",
    "def image_for_extraction(raw_image):\n",
    "\t\n",
    "\traw_image = cv2.GaussianBlur(raw_image,(3,3),0)\n",
    "\t\n",
    "\tret,no_sm_bw_image = cv2.threshold(raw_image,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\t\n",
    "\treturn no_sm_bw_image\n",
    "\t\n",
    "def getTransformationMatrix(img):\n",
    "\t#input should be a binarized image - text white, bg black\n",
    "\t\n",
    "\t#Find all white pixels\n",
    "\tpts = np.empty([0,0])\n",
    "\tpts = cv2.findNonZero(img)\n",
    "\n",
    "\t#Get rotated rect of white pixels\n",
    "\trect = cv2.minAreaRect(pts)\n",
    "\t\n",
    "\t# rect[0] has the center of rectangle, rect[1] has width and height, rect[2] has the angle\n",
    "\t# To draw the rotated box and save the png image, uncomment below\n",
    "\tdrawrect = img.copy()\n",
    "\tdrawrect = cv2.cvtColor(drawrect, cv2.COLOR_GRAY2BGR)\n",
    "\tbox = cv2.boxPoints(rect)\n",
    "\tbox = np.int0(box) # box now has four vertices of rotated rectangle\n",
    "\tcv2.drawContours(drawrect,[box],0,(0,0,255),10)\n",
    "\tcv2.imwrite('rotated_rect.png', drawrect)\n",
    "\n",
    "\t#Change rotation angle if the tilt is in another direction\n",
    "\trect = list(rect)\n",
    "\tif (rect[1][0] < rect[1][1]): # rect.size.width > rect.size.height\n",
    "\t\ttemp = list(rect[1])\n",
    "\t\ttemp[0], temp[1] = temp[1], temp[0]\n",
    "\t\trect[1] = tuple(temp)\n",
    "\t\trect[2] = rect[2] + 90.0\n",
    "\n",
    "\t#convert rect back to numpy/tuple\n",
    "\trect = np.asarray(rect)\n",
    "\t\n",
    "\t#Rotate the image according to the found angle\n",
    "\trotated_image = np.empty([0,0])\n",
    "\tM = cv2.getRotationMatrix2D(rect[0], rect[2], 1.0)\n",
    "\t#img = cv2.warpAffine(img, M, (img.shape[1],img.shape[0]))\n",
    "\n",
    "\t#returns the transformation matrix for this rotation\n",
    "\treturn M\n",
    "\n",
    "def rotate(image, M):\n",
    "\treturn cv2.warpAffine(image, M, (image.shape[1],image.shape[0]))\n",
    "\n",
    "def findLines(bw_image, LinesThres):\n",
    "\t# making horizontal projection\n",
    "\thorProj = cv2.reduce(bw_image, 100, cv2.REDUCE_AVG)\n",
    "\n",
    "\t# make hist - same dimension as horProj - if 0 (space), then True, else False\n",
    "\tth = 0; # black pixels threshold value. this represents the space lines\n",
    "\thist = horProj <= th;\n",
    "\n",
    "\t#Get mean coordinate of white white pixels groups\n",
    "\tycoords = []\n",
    "\ty = 0\n",
    "\tcount = 0\n",
    "\tisSpace = False\n",
    "\n",
    "\tfor i in range(0, bw_image.shape[0]):\n",
    "\t\t\n",
    "\t\tif (not isSpace):\n",
    "\t\t\tif (hist[i]): #if space is detected, get the first starting y-coordinates and start count at 1\n",
    "\t\t\t\tisSpace = True\n",
    "\t\t\t\tcount = 1\n",
    "\t\t\t\ty = i\n",
    "\t\telse:\n",
    "\t\t\tif (not hist[i]):\n",
    "\t\t\t\tisSpace = False\n",
    "\t\t\t\t#when smoothing, thin letters will breakdown, creating a new blank lines or pixel rows, but the count will be small, so we set a threshold.\n",
    "\t\t\t\tif (count >=LinesThres):\n",
    "\t\t\t\t\tycoords.append(y // count)\n",
    "\t\t\telse:\n",
    "\t\t\t\ty = y + i\n",
    "\t\t\t\tcount = count + 1\n",
    "\n",
    "\tycoords.append(y // count)\n",
    "\t\n",
    "\t#returns y-coordinates of the lines found\n",
    "\treturn ycoords\n",
    "\n",
    "def LinesMedian(bw_image):\n",
    "\t# making horizontal projections\n",
    "\t\n",
    "\thorProj = cv2.reduce(bw_image, 1, cv2.REDUCE_AVG)\n",
    "\n",
    "\t# make hist - same dimension as horProj - if 0 (space), then True, else False\n",
    "\tth = 0; # black pixels threshold value. this represents the space lines\n",
    "\thist = horProj <= th;\n",
    "\n",
    "\t#Get mean coordinate of white white pixels groups\n",
    "\tycoords = []\n",
    "\ty = 0\n",
    "\tcount = 0\n",
    "\tisSpace = False\n",
    "\tmedian_count = []\n",
    "\n",
    "\tfor i in range(0, bw_image.shape[0]):\n",
    "\t\t\n",
    "\t\tif (not isSpace):\n",
    "\t\t\tif (hist[i]): #if space is detected, get the first starting y-coordinates and start count at 1\n",
    "\t\t\t\tisSpace = True\n",
    "\t\t\t\tcount = 1\n",
    "\t\t\t\t#y = i\n",
    "\t\telse:\n",
    "\t\t\tif (not hist[i]):\n",
    "\t\t\t\tisSpace = False\n",
    "\t\t\t\tmedian_count.append(count)\n",
    "\t\t\telse:\n",
    "\t\t\t\t#y = y + i\n",
    "\t\t\t\tcount = count + 1\n",
    "\tmedian_count.append(count)\n",
    "\t#ycoords.append(y / count)\n",
    "\t\n",
    "\t#returns counts of each blank rows of each of the lines found\n",
    "\treturn median_count\n",
    "\n",
    "def get_lines_threshold(percent, img_for_det):\n",
    "\tThresPercent = percent\n",
    "\tLinMed = LinesMedian(img_for_det)\n",
    "\tLinMed = sorted(LinMed)\n",
    "\tLinesThres = LinMed[len(LinMed)//3]*(ThresPercent//100.0)\n",
    "\tLinesThres = int(LinesThres)\n",
    "\treturn LinesThres\n",
    "\n",
    "def findSpaces(line, thres_space):\n",
    "\t\n",
    "\t# making vertical projections\n",
    "\t\n",
    "\tverProj = cv2.reduce(line, 0, cv2.REDUCE_AVG)\n",
    "\t#print('v',verProj)\n",
    "\t# make hist - same dimension as horProj - if 0 (space), then True, else False\n",
    "\tth = 0 # black pixels threshold value. this represents the space lines\n",
    "\thist = (verProj <= th)\n",
    "\t#print('hist',hist)\n",
    "\t#Get mean coordinate of white white pixels groups\n",
    "\txcoords = []\n",
    "\tx = 0\n",
    "\tcount = 0\n",
    "\tisSpace = False\n",
    "\t#print('shape',line.shape[1])\n",
    "\tfor i in range(0, line.shape[1]):\n",
    "\t\tif (not isSpace):\n",
    "\t\t\tif (hist[0][i]): #if space is detected, get the first starting x-coordinates and start count at 1\n",
    "\t\t\t\t#print('hist i',hist[0][i],i)\n",
    "\t\t\t\tisSpace = True\n",
    "\t\t\t\tcount = 1\n",
    "\t\t\t\tx = i\n",
    "\t\telse:\n",
    "\t\t\tif (not hist[0][i]):\n",
    "\t\t\t\tisSpace = False\n",
    "\t\t\t\t#when smoothing, thin letters will breakdown, creating a new blank lines or pixel columns, but the count will be small, so we set a threshold.\n",
    "\t\t\t\t#print count,\"\\t\",\n",
    "\t\t\t\tif (count > thres_space):\n",
    "\t\t\t\t\txcoords.append(x // count)\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = x + i\n",
    "\t\t\t\tcount = count + 1\n",
    "\t\n",
    "\n",
    "\txcoords.append(x // count)\n",
    "\t\n",
    "\treturn xcoords\n",
    "\n",
    "def SpacesMedian(line):\n",
    "\t\n",
    "\t# making vertical projections\n",
    "\t\n",
    "\tverProj = cv2.reduce(line, 0, cv2.REDUCE_AVG)\n",
    "\n",
    "\t# make hist - same dimension as horProj - if 0 (space), then True, else False\n",
    "\tth = 0; # black pixels threshold value. this represents the space lines\n",
    "\thist = verProj <= th;\n",
    "\n",
    "\t#Get mean coordinate of white white pixels groups\n",
    "\txcoords = []\n",
    "\tx = 0\n",
    "\tcount = 0\n",
    "\tisSpace = False\n",
    "\tmedian_count = []\n",
    "\tfor i in range(0, line.shape[1]):\n",
    "\t\tif (not isSpace):\n",
    "\t\t\tif (hist[0][i]): #if space is detected, get the first starting x-coordinates and start count at 1\n",
    "\t\t\t\tisSpace = True\n",
    "\t\t\t\tcount = 1\n",
    "\t\t\t\t#x = i\n",
    "\t\telse:\n",
    "\t\t\tif (not hist[0][i]):\n",
    "\t\t\t\tisSpace = False\n",
    "\t\t\t\t#when smoothing, thin letters will breakdown, creating a new blank lines or pixel columns, but the count will be small, so we set a threshold.\n",
    "\t\t\t\t#print count,\"\\t\",\n",
    "\t\t\t\t\n",
    "\t\t\t\t#append each count of rows of blank gaps found\n",
    "\t\t\t\tmedian_count.append(count)\n",
    "\t\t\t\t\n",
    "\t\t\t\t#if (count > 15):\n",
    "\t\t\t\t\t#xcoords.append(x / count)\n",
    "\t\t\telse:\n",
    "\t\t\t\t#x = x + i\n",
    "\t\t\t\tcount = count + 1\n",
    "\t\n",
    "\tmedian_count.append(count)\n",
    "\txcoords.append(x // count)\n",
    "\t\n",
    "\t#returns x-coordinates of the spaces found in the line\n",
    "\treturn median_count\n",
    "\t\n",
    "def get_spaces_threshold(ycoords, img_for_det) :\n",
    "\n",
    "\t## Find Median for setting threshold\n",
    "\tmedianList = []\n",
    "\tfor i in range ( 0, len(ycoords)-1 ):\n",
    "\t\tline = img_for_det[range(ycoords[i],ycoords[i+1])]\n",
    "\t\tmedianList.append(SpacesMedian(line))\n",
    "\t\n",
    "\t#medianList contains count of each blank columns found in all lines\n",
    "\t#including spaces found between each characters too\n",
    "\t\n",
    "\t#find the row among medianList[] with maximum length\n",
    "\tmax_len = len(medianList[0])\n",
    "\tmax_in = 0 #for index number\n",
    "\tfor i in range (0, len(medianList)):\n",
    "\t\tif max_len < len(medianList[i]):\n",
    "\t\t\tmax_len = len(medianList[i])\n",
    "\t\t\tmax_in = i\n",
    "\n",
    "\t#sort the row  having the maximum no. of elements (decending order)\n",
    "\tmList = sorted(medianList[max_in],reverse=True)\n",
    "\t\n",
    "\t#delete elements produced from the page's margin\n",
    "\tmList = np.delete(mList, [0,1,2])\n",
    "\t\n",
    "\tfirstItem = mList[0]\n",
    "\tfor i in range (len(mList)-1, 0, -1):\n",
    "\t\tif mList[i] < firstItem//2:\n",
    "\t\t\tmList = np.delete(mList,i)\n",
    "\n",
    "\tmean = np.mean(mList)\n",
    "\tthreshold_space = mean/2\n",
    "\t\n",
    "\treturn threshold_space\n",
    "\n",
    "def get_words(raw_image):\n",
    "\t\n",
    "\t#Returns a list/array of all the words found along with the number of words on each line.\n",
    "\t\n",
    "\t#preprocessing of the image\n",
    "\t\n",
    "\t#img_for_det used for detecting the character and lines boundaries\n",
    "\timg_for_det = image_for_detection(raw_image)\n",
    "\t\n",
    "\t#img_for_ext used for the actual extraction of the characters\n",
    "\timg_for_ext = image_for_extraction(raw_image)\n",
    "\t\n",
    "\t#get the rotated angle of the tilt\n",
    "\tM = getTransformationMatrix(img_for_det) # M is transformation matrix\n",
    "\t#rotate the iamge with M\n",
    "\timg_for_det = rotate(img_for_det,M)\n",
    "\t#rotate image that will be used for extraction too\n",
    "\timg_for_ext = rotate(img_for_ext,M)\n",
    "\t\n",
    "\t#for debugging purpose, we also write the images to files\n",
    "\tcv2.imwrite('img_for_detection.png', img_for_det)\n",
    "\tcv2.imwrite('img_for_extraction.png', img_for_ext)\n",
    "\n",
    "\t#get threshold to determine how much gap should be considered as the line gap\n",
    "\tLinesThres = get_lines_threshold(50, img_for_det)\n",
    "\tycoords = findLines(img_for_det, LinesThres)\n",
    "\t#print(ycoords)\n",
    "\t# save image with lines printed ==========\n",
    "\timg_with_lines = img_for_ext.copy()\n",
    "\tfor i in ycoords:\n",
    "\t\tcv2.line(img_with_lines,(0,i),(img_with_lines.shape[1],i),255,1)\n",
    "\t#cv2.imwrite('img_with_lines.png', img_with_lines)\n",
    "\t#==========\n",
    "\n",
    "\t### =========== lines detection finish - ===========================\n",
    "\t\n",
    "\t#calculate max_line_height on each line\n",
    "\tmax_height_on_line = []\n",
    "\tfor i in range ( 0, len(ycoords)-1 ): #iterate line\n",
    "\t\t\n",
    "\t\tline = img_for_ext[range(ycoords[i],ycoords[i+1])]\n",
    "\n",
    "\t\t# to find max_line_height of each line we find contours again in this line only\n",
    "      \n",
    "\t\t#cnt_len = cv2.arcLength(cnt, True)\n",
    "\t\tcontour0 = cv2.findContours(line.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\t\tcontours = [cv2.approxPolyDP(cnt,2,True) for cnt in contour0[0]]\n",
    "\n",
    "        # === Extract Bounding Rectangles\n",
    "\t\tmaxArea = 0\n",
    "\t\trect=[]\n",
    "\t\tfor ctr in contours:\n",
    "\t\t\tmaxArea = max(maxArea,cv2.contourArea(ctr))\n",
    "\n",
    "\t\tareaRatio = 0.008\n",
    "\n",
    "\t\tfor ctr in contours:\n",
    "\t\t\tif cv2.contourArea(ctr) > maxArea * areaRatio: \n",
    "\t\t\t\trect.append(cv2.boundingRect(cv2.approxPolyDP(ctr,1,True)))\n",
    "\t\t\n",
    "\t\t#Find max_line_height and width\n",
    "\t\tmax_line_height = 0\n",
    "\t\t\n",
    "\t\t#for x in rect:\n",
    "\t\t#\t\tround(x)\n",
    "\t\t#rect.append(x)\n",
    "\t\tfor i in rect:\n",
    "\t\t\tx = i[0]\n",
    "\t\t\ty = i[1]\n",
    "\t\t\tw = i[2]\n",
    "\t\t\th = i[3]\n",
    "\t\t\t\n",
    "\t\t\tif(h>max_line_height):\n",
    "\t\t\t\tmax_line_height = h\n",
    "\t\t\n",
    "\t\tmax_height_on_line.append(max_line_height)\n",
    "\t\n",
    "\t### =========== space in a line detection begins ===================\n",
    "\n",
    "\t#get the threshold to determine how much gap should be considered as the space between the words\n",
    "\tthreshold_space = get_spaces_threshold(ycoords, img_for_det)\n",
    "\n",
    "\t#split lines based on the ycoords of the detected lines\n",
    "\t#each line is put into the var 'line' and the words are found\n",
    "\t#based on the threshold_space value.\n",
    "\n",
    "\twords_on_line=[]\n",
    "\tall_words=[]\n",
    "\tcount = 0\n",
    "\tnumber_of_words = 0\n",
    "\n",
    "\tfor i in range ( 0, len(ycoords)-1 ): #iterate line\n",
    "\n",
    "\t\tline = img_for_det[range(ycoords[i],ycoords[i+1])]\n",
    "\t\t#cv2.imwrite(os.path.join(\"C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\out\\\\\",'line_no'+str(i)+'.png', line))\n",
    "\t\t\n",
    "\t\t#finding the x-coordinates of the spaces\n",
    "\t\txcoords = findSpaces(line, threshold_space)\n",
    "\t\t\n",
    "\t\t#print((xcoords))\n",
    "\t\tfor x in xcoords:\n",
    "\t\t\tcv2.line(line, (x,0), (x,line.shape[0]), 255, 1)\n",
    "\t\t#cv2.imwrite('img/'+str(i)+'.png', line)\n",
    "\t\tcv2.imwrite(os.path.join(\"C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\out1\\\\\" , str(i)+ \".png\"),~line)\n",
    "\t\tcount = 0\n",
    "\t\t\n",
    "\t\tfor j in range (0, len(xcoords)-1 ): #iterate words\n",
    "\t\t\t\n",
    "\t\t\t#use image with no smoothing\n",
    "\t\t\tline = img_for_ext[range(ycoords[i],ycoords[i+1])]\n",
    "\t\t\t#print('line',line)\n",
    "\t\t\tword = line[:, xcoords[j]: xcoords[j+1]]\n",
    "\t\t\t#print(word)\n",
    "\t\t\tall_words.append(word)\n",
    "\t\t\t#cv2.imwrite('img/words/'+str(number_of_words)+'.png', word)\n",
    "\t\t\tcv2.imwrite(os.path.join(\"C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\out2\\\\\" , str(number_of_words)+ \".png\"),~word)\n",
    "\t\t\tcount = count + 1\n",
    "\t\t\tnumber_of_words = number_of_words + 1\n",
    "\t\t\t#Generate space here\n",
    "\t\t\n",
    "\t\twords_on_line.append(count)\n",
    "\t\t# Line Change\n",
    "\t\n",
    "\treturn all_words, words_on_line, max_height_on_line\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def image_segmentation1(name1):\n",
    "    raw_image = cv2.imread(name1,0)\n",
    "    count=0\n",
    "        #get all the words (as an numpy image array), words on each line, and maximum height on that line\n",
    "    all_words, words_on_line, max_height_on_line = get_words(raw_image)\n",
    "    #for i in range(0, len(words_on_line)):\n",
    "    #    for j in range(0, words_on_line[i]):\n",
    "    #        all_characters = get_characters(all_words[count],max_height_on_line[i],i,j)\n",
    "    #        cv2.imshow(\"all_words[count]\",all_words[count])\n",
    "    #        count = count + 1\n",
    "    print (\"no. of lines = \",len(words_on_line))\n",
    "    print (words_on_line)\n",
    "    print (\"no. of words = \",len(all_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big_merged.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    #\"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    #\"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    #\"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    #\"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    #\"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "    \n",
    "def edits2(word): \n",
    "    #\"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to resize the image into appropriate dimensions\n",
    "def resize(img):\n",
    "    img = cv2.resize(img,(20,20))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Openvoice():\n",
    "    import os \n",
    "\n",
    "    # The text that you want to convert to audio \n",
    "    #file_name='/home/anu/PROJECT/OCR-MODEL-master/extract/letter/'+ text_file\n",
    "    #with open(file_name,'r') as myfile:\n",
    "    #    data=myfile.read().replace('\\n','')\n",
    "    str1 = ' '.join(words)\n",
    "    mytext=str1\n",
    "\n",
    "    print(mytext)\n",
    "    # Language in which you want to convert \n",
    "    language = 'en'\n",
    " \n",
    "    myobj = gTTS(text=mytext, lang=language, slow=False) \n",
    "    \n",
    "    import time\n",
    "    #timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    myobj.save(\"C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\out\\\\\"+ timestr+'.mp3') \n",
    "\n",
    "    #print timestr\n",
    "    #pygame.mixer.init()\n",
    "    #pygame.mixer.music.load('/home/anu/PROJECT/OCR-MODEL-master/extract/output/' + timestr +'.mp3')\n",
    "    #pygame.mixer.music.play()\n",
    "\n",
    "    # Saving the converted audio in a mp3 file named \n",
    "    # welcome \n",
    "    # Playing the converted file \n",
    "    #os.system(\"welcome.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict():    \n",
    "    listd=[]\n",
    "    new=[]\n",
    "    global words\n",
    "    global ans\n",
    "\n",
    "    words=[]\n",
    "    path='C:\\\\Users\\HP\\Desktop\\optical-character-recognition-OCR-master\\out2/'\n",
    "    for image in listdir(path):\n",
    "        listd.append(image)\n",
    "    res = [int(sub.split('.')[0]) for sub in listd] \n",
    "    res.sort()\n",
    "\n",
    "    for image_name in res:\n",
    "        a=str(image_name)+'.png'\n",
    "        new.append(a)\n",
    "    #print(new)\n",
    "\n",
    "\n",
    "    for image_name in new:\n",
    "        counter = 1\n",
    "        #print(image_name)\n",
    "        # constructing the name of the file \n",
    "        file_name = 'C:\\\\Users\\HP\\Desktop\\optical-character-recognition-OCR-master\\out2/' + image_name\n",
    "\n",
    "        # getting segmented images \n",
    "        letters_in_image = image_segmentation(file_name)\n",
    "\n",
    "        # sorting the letters so that letters that appear before is addressed first \n",
    "        letters_in_image = sorted(letters_in_image, key=lambda x: x[0])\n",
    "\n",
    "        ans = \"\"\n",
    "        ans1=\"\"\n",
    "        for (x,y,w,h) in letters_in_image:\n",
    "            image = cv2.imread(file_name,0)\n",
    "            letter = image[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "\n",
    "            #cv2.imwrite(str(counter)+'.jpg', letter)\n",
    "            counter = counter + 1\n",
    "\n",
    "            letter  = resize(letter)/255\n",
    "            X_test = np.asarray(letter)\n",
    "            X_test = np.reshape(X_test, [-1,20,20,1])\n",
    "            output = [np.argmax(model.predict(X_test, verbose = 0))]\n",
    "            output = label_encoder.inverse_transform(output)\n",
    "            ans1+=output[0].lower()\n",
    "            ans=correction(ans1)\n",
    "        words.append(ans)\n",
    "        PathTextBox2.insert(END, \" \" + ans)\n",
    "    #print(\" \", ans)\n",
    "    print(words)\n",
    "    \n",
    "    \n",
    "    path='C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\text\\\\'\n",
    "    for i in words:\n",
    "        global text_file\n",
    "        import time\n",
    "        #timestr1 = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        text_file = open(path+timestr+'.txt', \"a\")\n",
    "        text_file.write(i)\n",
    "        text_file.write(' ')\n",
    "        text_file.close()\n",
    "    Openvoice()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def stop():\n",
    "    pygame.mixer.music.pause()\n",
    "    \n",
    "def play():\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load('C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\out\\\\' + timestr +'.mp3')\n",
    "    pygame.mixer.music.play()\n",
    "        \n",
    "def Image12():\n",
    "    print('in image')\n",
    "    img=cv2.imread(name1)\n",
    "    cv2.imshow(\"Input Image\",img)\n",
    "\n",
    "def clear():\n",
    "    PathTextBox2.delete(\"1.0\",END)\n",
    "    PathTextBox.delete(\"1.0\",END)\n",
    "    for root, dirs, files in os.walk('C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\out2\\\\'):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "        for d in dirs:\n",
    "            shutil.rmtree(os.path.join(root, d))\n",
    "\n",
    "    for root, dirs, files in os.walk('C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\out1\\\\'):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "        for d in dirs:\n",
    "            shutil.rmtree(os.path.join(root, d))\n",
    "    import time\n",
    "    global timestr\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    print(timestr, 'clear 1')\n",
    "    \n",
    "def OpenTxt():\n",
    "    filename='C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\text\\\\'+timestr+'.txt'\n",
    "\n",
    "    #from subprocess import call\n",
    "    #call(\"/home/anu/PROJECT/OCR-MODEL-master/extract/letter/20200427-201115.txt\")\n",
    "    try:\n",
    "        os.system( '{0} \"{1}\"'.format( os.environ['EDITOR'], filename ))\n",
    "    except:\n",
    "         os.system( 'notepad \"{0}\"'.format(filename) )\n",
    "    \n",
    "\n",
    "def OpenTxt_B():\n",
    "    filename='C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\text2\\\\'+timestr+'.txt'\n",
    "\n",
    "    #from subprocess import call\n",
    "    #call(\"/home/anu/PROJECT/OCR-MODEL-master/extract/letter/20200427-201115.txt\")\n",
    "    try:\n",
    "        os.system( '{0} \"{1}\"'.format( os.environ['EDITOR'], filename ))\n",
    "    except:\n",
    "         os.system( 'notepad \"{0}\"'.format(filename) )    \n",
    "    \n",
    "def translate():\n",
    "    import re\n",
    "    trans=[]\n",
    "    braille = '⠁⠃⠉⠙⠑⠋⠛⠓⠊⠚⠅⠇⠍⠝⠕⠏⠟⠗⠎⠞⠥⠧⠺⠭⠽⠵' '⠁⠃⠉⠙⠑⠋⠛⠓⠊⠚'\n",
    "    normal  = 'abcdefghijklmnopqrstuvwxyz' '1234567890'\n",
    "    table = str.maketrans(normal, braille) # define the translation table\n",
    "\n",
    "    def make_braille(data):\n",
    "        data = re.sub(r'(\\d+)', r'⠼\\1', data) # add ⠼ before every number\n",
    "        data = data.translate(table) # convert the remaining characters to braille\n",
    "        return data\n",
    "    print(\"In translate\")\n",
    "    \n",
    "    for i in words:\n",
    "        temp=make_braille(i)\n",
    "        \n",
    "        trans.append(temp)\n",
    "        print(temp)\n",
    "    path='C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\text2\\\\'\n",
    "    for i in trans:\n",
    "        global text_file\n",
    "        import time\n",
    "        #timestr1 = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        text_file = open(path+timestr+'.txt', \"a\", encoding=\"utf-8\")\n",
    "        text_file.write(i)\n",
    "        text_file.write(' ')\n",
    "        text_file.close() \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "def OpenFile():\n",
    "    global letter_images\n",
    "    global name1\n",
    "    name1 = askopenfilename(initialdir=\"C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\images\\\\\",\n",
    "                           filetypes =((\"PNG File\", \"*.png\"),(\"BMP File\", \"*.bmp\"),(\"JPG File\", \"*.jpg\"),(\"JPEG File\", \"*.jpeg\")),\n",
    "                           title = \"Choose a file.\"\n",
    "                           ) \n",
    "    image_segmentation1(name1)\n",
    "    PathTextBox.delete(\"1.0\",END)\n",
    "    PathTextBox.insert(END,name1)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200918-225607 gui time\n",
      "no. of lines =  3\n",
      "[4, 6, 5]\n",
      "no. of words =  15\n",
      "['ducal', 'character', 'cognition', 'project', 'the', 'dentfcaton', 'ok', 'printed', 'characters', 'using', 'photoelectric', 'device', 'and', 'computer', 'sore']\n",
      "ducal character cognition project the dentfcaton ok printed characters using photoelectric device and computer sore\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageTk,Image\n",
    "from tkinter import ttk\n",
    "from ttkthemes import ThemedStyle\n",
    "import time\n",
    "global timestr\n",
    "root = Tk(  )\n",
    "root.resizable(0,0)      #will disable max/min tab of window\n",
    "root.configure(bg=\"#808080\")\n",
    "location=str()\n",
    "#root.geometry(\"3000X2000\")\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(timestr,'gui time')\n",
    "im = Image.open('ocrm.png')\n",
    "ph = ImageTk.PhotoImage(im)\n",
    "#plt.imshow(ph)\n",
    "\n",
    "Title = root.title( \"Optical Character Recognition Model\")\n",
    "path = StringVar()\n",
    "\n",
    "#s = ttk.Style()\n",
    "#s.configure('tab1.TFrame', background='#BB8FCE', foreground='blue')\n",
    "about= \" OCR model is conversion of images of typed or printed text into machine-encoded text, \\n whether from a scanned document, photo of a document, a scene-photo (for example the \\ntext on signs and billboards in a landscape photo). OCR is a field of research in pattern \\n recognition, artificial intelligence and computer vision. The scanned page of a physical\\n document can be displayed on the screen and can be read, but for the computer, it is \\njust a series of black and white dots, which it cannot recognise. To enable the computer\\n to read a scanned document and produce a soft copy, OCR was developed. It examines\\ntext of a scanned document and translates the characters into code that makes the text \\nmachine-readable so that it can be transformed into an electronic format or soft copy just\\n like a document created with a word processor which users can edit, format, search\\n and read.Thus, it helps computer recognize words and characters on a scanned page\\n or digital images of physical printed or handwritten documents by using the optical \\nproperties of words and characters printed on a scanned page or document.\"\n",
    "contact= \"A. P. SHAH INSTITUTE OF TECHNOLOGY\\n\\n\\t PROJECT MADE BY:\\n\\n\\t ANUJA VELASKAR\\n\\n\\t NIDHI MUNAVALLI\\n\\n            APURVA WAINGANKAR\"\n",
    "#ADING TABSThe scanned page of a physical document can be displayed on the screen and can be read, but for the computer, it is just a series of black and white dots, which it cannot recognise. To enable the computer to read a scanned document and produce a soft copy, OCR was developed. OCR examines the text of a scanned document and translates the characters into code that makes the text machine-readable so that it can be transformed into an electronic format or soft copy just like a document created with a word processor which users can edit, format, search and read.\n",
    "\n",
    "\n",
    "#ADING TABS\n",
    "TAB_CONTROL = ttk.Notebook(root)\n",
    "\n",
    "\n",
    "style = ttk.Style()\n",
    "style = ThemedStyle(TAB_CONTROL)\n",
    "style.set_theme(\"radiance\")\n",
    "\n",
    "style.configure(\"TNotebook.Tab\", background=\"#6B6A6A\", foreground=\"#6C7907\", borderwidth=10)\n",
    "style.configure('tab1.TFrame',expand=\"full\", foreground='blue')\n",
    "\n",
    "style.configure('TButton', background=\"#8EA002\", foreground=\"black\")\n",
    "style.configure('TLabel', foreground=\"black\")\n",
    "\n",
    "TAB1 = ttk.Frame(TAB_CONTROL, style = 'tab1.TFrame')\n",
    "TAB_CONTROL.add(TAB1, text= 'HOME')\n",
    "\n",
    "TAB2 = ttk.Frame(TAB_CONTROL, style = 'tab1.TFrame')\n",
    "TAB_CONTROL.add(TAB2, text='INPUT WINDOW')\n",
    "\n",
    "TAB3 = ttk.Frame(TAB_CONTROL, style = 'tab1.TFrame')\n",
    "TAB_CONTROL.add(TAB3, text='OUTPUT WINDOW')\n",
    "\n",
    "TAB4 = ttk.Frame(TAB_CONTROL, style = 'tab1.TFrame')\n",
    "TAB_CONTROL.add(TAB4, text='ABOUT OCR MODEL')\n",
    "\n",
    "TAB5 = ttk.Frame(TAB_CONTROL, style = 'tab1.TFrame')\n",
    "TAB_CONTROL.add(TAB5, text='CONTACT US')\n",
    "\n",
    "TAB_CONTROL.pack(expand=1, fill=\"both\")\n",
    "\n",
    "ttk.Label(TAB4, text= about, style=\"TLabel\", foreground=\"#3C4C00\").pack(padx=5, pady=20)\n",
    "ttk.Label(TAB5, text= contact, style=\"TLabel\",foreground=\"#3C4C00\").pack(padx=30, pady=50)\n",
    "\n",
    "\n",
    "#tkinter.lab=Label(TAB1,image=ph).place(x=100,y=70)\n",
    "\n",
    "#TAB_CONTROL.add(TAB1, text=\"profile\", image=ph)#, compound=tk.TOP) # use the tk constants\n",
    "try:\n",
    "    lab=Label(TAB1,image=ph).place(x=60,y=45)\n",
    "except TclError:\n",
    "    ttk.Label(TAB5, text= home, style=\"TLabel\").pack(anchor=CENTER,padx=30, pady=50)\n",
    "\n",
    "#creating another frames\n",
    "BTAB3 = ttk.Frame(TAB3, style = 'tab1.TFrame')\n",
    "BTAB3.pack(side=BOTTOM, expand=1,pady=10)\n",
    "TTAB3= ttk.Frame(TAB3, style = 'tab1.TFrame')\n",
    "TTAB3.pack(side=TOP, expand=1)\n",
    "CTAB3= ttk.Frame(TAB3, style = 'tab1.TFrame')\n",
    "CTAB3.pack(side=TOP, expand=1)\n",
    "\n",
    "InputLabel = ttk.Label(TAB2,text = \"INPUT IMAGE\",style=\"TLabel\")\n",
    "InputLabel.pack(side= TOP, pady=15)\n",
    "BrowseButton = ttk.Button(TAB2,text=\"Browse\",style=\"TButton\",command = OpenFile)\n",
    "BrowseButton.pack(side = TOP, pady=10,padx=10)\n",
    "\n",
    "PathTextBox = Text(TAB2,width = 60,height= 2,bg=\"#EEF0C3\", fg=\"black\")\n",
    "PathTextBox.pack(side = TOP, fill = X,padx=10)\n",
    "SubmitButton = ttk.Button(TAB2,text=\"Submit\",style=\"TButton\",command = predict)\n",
    "SubmitButton.pack(side = TOP, padx=10, pady =15) \n",
    "ClearButton = ttk.Button(TAB2,text=\"Start Over\",style=\"TButton\",command = clear)\n",
    "ClearButton.pack(side = TOP, pady=20,padx=10)\n",
    "\n",
    "PathLabel = ttk.Label(TTAB3,text = \"Text \",style=\"TLabel\")\n",
    "PathLabel.pack(pady=7)\n",
    "PathTextBox2 = Text(TTAB3,width = 60, height=3,bg=\"#EEF0C3\", fg=\"black\")\n",
    "PathTextBox2.pack(side = TOP, fill = X,pady=10)\n",
    "                \n",
    "TxtLabel = ttk.Label(BTAB3,text = \"Open Image                                   English Text File\",style=\"TLabel\")\n",
    "TxtLabel.pack(pady=10)\n",
    "ReadButton = ttk.Button(BTAB3, style=\"TButton\",text=\"Image\",command = Image12 )#Opens location where voice file is being stored\n",
    "ReadButton.pack(padx=50, side=LEFT)\n",
    "TxtButton = ttk.Button(BTAB3,text=\"Open\",style=\"TButton\",command = OpenTxt )#Opens location where voice file is being stored\n",
    "TxtButton.pack(padx=50,side=LEFT)\n",
    "\n",
    "PathLabel = ttk.Label(CTAB3,text = \"Convert to Braille: \",style=\"TLabel\")\n",
    "PathLabel.pack(pady=10)\n",
    "TButton = ttk.Button(CTAB3,text=\"Translate\",style=\"TButton\",command = translate )#Opens location where voice file is being stored\n",
    "TButton.pack(padx=50,side=LEFT)\n",
    "TxtButton = ttk.Button(CTAB3,text=\"Open File\",style=\"TButton\",command = OpenTxt_B )#Opens location where voice file is being stored\n",
    "TxtButton.pack(padx=50,side=LEFT)\n",
    "\n",
    "PathLabel = ttk.Label(TAB3,text = \"Listen To Speech: \",style=\"TLabel\")\n",
    "PathLabel.pack(pady=10)\n",
    "SpeechButton = ttk.Button(TAB3,style=\"TButton\",text=\"Play\",command = play )#Opens location where voice file is being stored\n",
    "SpeechButton.pack(padx=125,side=LEFT)\n",
    "StopButton = ttk.Button(TAB3,text=\"Stop\",style=\"TButton\",command = stop )#Opens location where voice file is being stored\n",
    "StopButton.pack(side=LEFT)\n",
    "\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install -c conda-forge pygobject \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
