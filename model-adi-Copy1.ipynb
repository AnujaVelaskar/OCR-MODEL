{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as k\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense,Dropout,Activation\n",
    "from keras.optimizers import Adam\n",
    "#from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "import argparse\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "#from pyimagesearch.stridednet import StridedNet\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img):\n",
    "    img = cv2.resize(img,(20,20))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# to get the name of the folder\n",
    "for name_folder in os.listdir(\"C:\\\\Users\\AUTOBOTS\\Downloads\\optical-character-recognition-OCR-master\\Fnt\") :\n",
    "\n",
    "    name = 'C:\\\\Users\\AUTOBOTS\\Downloads\\optical-character-recognition-OCR-master\\Fnt/' + name_folder\n",
    "    for f in listdir(name):\n",
    "        # name of the folder is the name of the output\n",
    "        y_train.append(np.asarray(name_folder))\n",
    "        \n",
    "        # constructing full path to the image\n",
    "        name = 'C:\\\\Users\\AUTOBOTS\\Downloads\\optical-character-recognition-OCR-master\\Fnt/' + name_folder + '/' + f\n",
    "        \n",
    "        # reading the image\n",
    "        image = cv2.imread(name,0)/255\n",
    "        \n",
    "        # appending to form the image list\n",
    "        image = np.asarray(image)\n",
    "        image = resize(image)\n",
    "        X_train.append([image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36560, 1, 20, 20)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, [-1,20,20,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36560, 20, 20, 1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y_train = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StridedNet:\n",
    "    def build(width, height, depth,reg, init=\"he_normal\"):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        \n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "# our first CONV layer will learn a total of 16 filters, each\n",
    "\t\t# Of which are 7x7 -- we'll then apply 2x2 strides to reduce\n",
    "\t\t# the spatial dimensions of the volume\n",
    "        model.add(Conv2D(16, (7, 7), strides=(2, 2), padding=\"valid\",\n",
    "            kernel_initializer=init, kernel_regularizer=reg,\n",
    "            input_shape=inputShape))\n",
    "        \n",
    "\t\t# here we stack two CONV layers on top of each other where\n",
    "\t\t# each layerswill learn a total of 32 (3x3) filters\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\",\n",
    "            kernel_initializer=init, kernel_regularizer=reg))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\",\n",
    "            kernel_initializer=init, kernel_regularizer=reg))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # stack two more CONV layers, keeping the size of each filter\n",
    "\t\t# as 3x3 but increasing to 64 total learned filters\n",
    "        model.add(Conv2D(64, (3, 3), padding=\"same\",\n",
    "            kernel_initializer=init, kernel_regularizer=reg))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\",\n",
    "            kernel_initializer=init, kernel_regularizer=reg))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "\t\t# increase the number of filters again, this time to 128\n",
    "        model.add(Conv2D(128, (3, 3), padding=\"same\",\n",
    "            kernel_initializer=init, kernel_regularizer=reg))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\",\n",
    "            kernel_initializer=init, kernel_regularizer=reg))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # fully-connected layer\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, kernel_initializer=init))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "\t\t# softmax classifier\n",
    "        model.add(Dense(36, activation=\"softmax\"))\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ap = argparse.ArgumentParser()\n",
    "#args = vars(ap.parse_args())\n",
    "    \n",
    "\n",
    "#opt = Adam(lr=1e-4, decay=1e-4 / args[50])\n",
    "model = StridedNet.build(width=20, height=20, depth=1,reg=l2(0.0005))\n",
    "#model=StridedNet()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\",metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 7, 7, 16)          800       \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 7, 7, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 2, 2, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 1, 1, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 36)                18468     \n",
      "=================================================================\n",
      "Total params: 379,908\n",
      "Trainable params: 377,988\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train,y_train ,test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29248, 36)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36560\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "121/121 [==============================] - 14s 116ms/step - loss: 4.6651 - accuracy: 0.0485 - val_loss: 3.9540 - val_accuracy: 0.0842\n",
      "Epoch 2/50\n",
      "121/121 [==============================] - 14s 118ms/step - loss: 3.7472 - accuracy: 0.1089 - val_loss: 4.0082 - val_accuracy: 0.1630\n",
      "Epoch 3/50\n",
      "121/121 [==============================] - 17s 137ms/step - loss: 3.2648 - accuracy: 0.1768 - val_loss: 3.3554 - val_accuracy: 0.2931\n",
      "Epoch 4/50\n",
      "121/121 [==============================] - 15s 127ms/step - loss: 2.8992 - accuracy: 0.2503 - val_loss: 2.5281 - val_accuracy: 0.4182\n",
      "Epoch 5/50\n",
      "121/121 [==============================] - 17s 144ms/step - loss: 2.6106 - accuracy: 0.3208 - val_loss: 1.9728 - val_accuracy: 0.5161\n",
      "Epoch 6/50\n",
      "121/121 [==============================] - 18s 146ms/step - loss: 2.3963 - accuracy: 0.3766 - val_loss: 1.9694 - val_accuracy: 0.5070\n",
      "Epoch 7/50\n",
      "121/121 [==============================] - 17s 144ms/step - loss: 2.2078 - accuracy: 0.4275 - val_loss: 1.7079 - val_accuracy: 0.6008\n",
      "Epoch 8/50\n",
      "121/121 [==============================] - 17s 141ms/step - loss: 2.0741 - accuracy: 0.4626 - val_loss: 1.3345 - val_accuracy: 0.6896\n",
      "Epoch 9/50\n",
      "121/121 [==============================] - 18s 145ms/step - loss: 1.9158 - accuracy: 0.5070 - val_loss: 1.2241 - val_accuracy: 0.7403\n",
      "Epoch 10/50\n",
      "121/121 [==============================] - 18s 148ms/step - loss: 1.8021 - accuracy: 0.5394 - val_loss: 1.1466 - val_accuracy: 0.7531\n",
      "Epoch 11/50\n",
      "121/121 [==============================] - 18s 150ms/step - loss: 1.7081 - accuracy: 0.5659 - val_loss: 1.0442 - val_accuracy: 0.7700\n",
      "Epoch 12/50\n",
      "121/121 [==============================] - 19s 153ms/step - loss: 1.6144 - accuracy: 0.5912 - val_loss: 1.0184 - val_accuracy: 0.7702\n",
      "Epoch 13/50\n",
      "121/121 [==============================] - 19s 155ms/step - loss: 1.5434 - accuracy: 0.6147 - val_loss: 0.9135 - val_accuracy: 0.8155\n",
      "Epoch 14/50\n",
      "121/121 [==============================] - 19s 153ms/step - loss: 1.4817 - accuracy: 0.6301 - val_loss: 0.9123 - val_accuracy: 0.8087\n",
      "Epoch 15/50\n",
      "121/121 [==============================] - 18s 149ms/step - loss: 1.4372 - accuracy: 0.6446 - val_loss: 0.8671 - val_accuracy: 0.8199\n",
      "Epoch 16/50\n",
      "121/121 [==============================] - 18s 148ms/step - loss: 1.3834 - accuracy: 0.6570 - val_loss: 0.8548 - val_accuracy: 0.8266\n",
      "Epoch 17/50\n",
      "121/121 [==============================] - 17s 142ms/step - loss: 1.3202 - accuracy: 0.6768 - val_loss: 0.8081 - val_accuracy: 0.8411\n",
      "Epoch 18/50\n",
      "121/121 [==============================] - 18s 145ms/step - loss: 1.2844 - accuracy: 0.6879 - val_loss: 0.8288 - val_accuracy: 0.8285\n",
      "Epoch 19/50\n",
      "121/121 [==============================] - 18s 150ms/step - loss: 1.2420 - accuracy: 0.7008 - val_loss: 0.7354 - val_accuracy: 0.8500\n",
      "Epoch 20/50\n",
      "121/121 [==============================] - 18s 146ms/step - loss: 1.2054 - accuracy: 0.7081 - val_loss: 0.8115 - val_accuracy: 0.8237\n",
      "Epoch 21/50\n",
      "121/121 [==============================] - 18s 146ms/step - loss: 1.1847 - accuracy: 0.7152 - val_loss: 0.7079 - val_accuracy: 0.8636\n",
      "Epoch 22/50\n",
      "121/121 [==============================] - 18s 147ms/step - loss: 1.1496 - accuracy: 0.7261 - val_loss: 0.7383 - val_accuracy: 0.8541\n",
      "Epoch 23/50\n",
      "121/121 [==============================] - 18s 146ms/step - loss: 1.1209 - accuracy: 0.7317 - val_loss: 0.6504 - val_accuracy: 0.8803\n",
      "Epoch 24/50\n",
      "121/121 [==============================] - 17s 143ms/step - loss: 1.1092 - accuracy: 0.7351 - val_loss: 0.6499 - val_accuracy: 0.8779\n",
      "Epoch 25/50\n",
      "121/121 [==============================] - 19s 158ms/step - loss: 1.0871 - accuracy: 0.7409 - val_loss: 0.6740 - val_accuracy: 0.8710\n",
      "Epoch 26/50\n",
      "121/121 [==============================] - 19s 154ms/step - loss: 1.0680 - accuracy: 0.7449 - val_loss: 0.6103 - val_accuracy: 0.8851\n",
      "Epoch 27/50\n",
      "121/121 [==============================] - 22s 180ms/step - loss: 1.0370 - accuracy: 0.7541 - val_loss: 0.6240 - val_accuracy: 0.8790\n",
      "Epoch 28/50\n",
      "121/121 [==============================] - 20s 164ms/step - loss: 1.0236 - accuracy: 0.7590 - val_loss: 0.6278 - val_accuracy: 0.8809\n",
      "Epoch 29/50\n",
      "121/121 [==============================] - 20s 163ms/step - loss: 1.0034 - accuracy: 0.7657 - val_loss: 0.5816 - val_accuracy: 0.8917\n",
      "Epoch 30/50\n",
      "121/121 [==============================] - 18s 152ms/step - loss: 0.9953 - accuracy: 0.7657 - val_loss: 0.5582 - val_accuracy: 0.9007\n",
      "Epoch 31/50\n",
      "121/121 [==============================] - 19s 155ms/step - loss: 0.9854 - accuracy: 0.7681 - val_loss: 0.5896 - val_accuracy: 0.8881\n",
      "Epoch 32/50\n",
      "121/121 [==============================] - 19s 156ms/step - loss: 0.9715 - accuracy: 0.7690 - val_loss: 0.7206 - val_accuracy: 0.8504\n",
      "Epoch 33/50\n",
      "121/121 [==============================] - 20s 165ms/step - loss: 0.9488 - accuracy: 0.7751 - val_loss: 0.5578 - val_accuracy: 0.8995\n",
      "Epoch 34/50\n",
      "121/121 [==============================] - 18s 149ms/step - loss: 0.9574 - accuracy: 0.7739 - val_loss: 0.5333 - val_accuracy: 0.9013\n",
      "Epoch 35/50\n",
      "121/121 [==============================] - 20s 167ms/step - loss: 0.9470 - accuracy: 0.7779 - val_loss: 0.5832 - val_accuracy: 0.8950\n",
      "Epoch 36/50\n",
      "121/121 [==============================] - 18s 150ms/step - loss: 0.9226 - accuracy: 0.7840 - val_loss: 0.5105 - val_accuracy: 0.9107\n",
      "Epoch 37/50\n",
      "121/121 [==============================] - 19s 154ms/step - loss: 0.9211 - accuracy: 0.7837 - val_loss: 0.5287 - val_accuracy: 0.9052\n",
      "Epoch 38/50\n",
      "121/121 [==============================] - 19s 156ms/step - loss: 0.9061 - accuracy: 0.7897 - val_loss: 0.5517 - val_accuracy: 0.8966\n",
      "Epoch 39/50\n",
      "121/121 [==============================] - 18s 153ms/step - loss: 0.9025 - accuracy: 0.7897 - val_loss: 0.5840 - val_accuracy: 0.8922\n",
      "Epoch 40/50\n",
      "121/121 [==============================] - 18s 147ms/step - loss: 0.8922 - accuracy: 0.7922 - val_loss: 0.5168 - val_accuracy: 0.9070\n",
      "Epoch 41/50\n",
      "121/121 [==============================] - 21s 170ms/step - loss: 0.8884 - accuracy: 0.7946 - val_loss: 0.5236 - val_accuracy: 0.9026\n",
      "Epoch 42/50\n",
      "121/121 [==============================] - 19s 155ms/step - loss: 0.8818 - accuracy: 0.7956 - val_loss: 0.5007 - val_accuracy: 0.9088\n",
      "Epoch 43/50\n",
      "121/121 [==============================] - 19s 153ms/step - loss: 0.8824 - accuracy: 0.7950 - val_loss: 0.5427 - val_accuracy: 0.9022\n",
      "Epoch 44/50\n",
      "121/121 [==============================] - 20s 163ms/step - loss: 0.8553 - accuracy: 0.8009 - val_loss: 0.5189 - val_accuracy: 0.9019\n",
      "Epoch 45/50\n",
      "121/121 [==============================] - 19s 157ms/step - loss: 0.8526 - accuracy: 0.8033 - val_loss: 0.6966 - val_accuracy: 0.8567\n",
      "Epoch 46/50\n",
      "121/121 [==============================] - 20s 168ms/step - loss: 0.8515 - accuracy: 0.8068 - val_loss: 0.5039 - val_accuracy: 0.9127\n",
      "Epoch 47/50\n",
      "121/121 [==============================] - 19s 154ms/step - loss: 0.8387 - accuracy: 0.8077 - val_loss: 0.4933 - val_accuracy: 0.9104\n",
      "Epoch 48/50\n",
      "121/121 [==============================] - 19s 161ms/step - loss: 0.8396 - accuracy: 0.8058 - val_loss: 0.5209 - val_accuracy: 0.9091\n",
      "Epoch 49/50\n",
      "121/121 [==============================] - 19s 161ms/step - loss: 0.8348 - accuracy: 0.8092 - val_loss: 0.4852 - val_accuracy: 0.9144\n",
      "Epoch 50/50\n",
      "121/121 [==============================] - 19s 158ms/step - loss: 0.8211 - accuracy: 0.8085 - val_loss: 0.5298 - val_accuracy: 0.9034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1cb41a29d48>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
    "\t  width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
    "\t  horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "model.fit_generator(aug.flow(X_train, y_train, batch_size=300),\n",
    "                             steps_per_epoch=len(X_train) // 300,\n",
    "                             epochs=50,\n",
    "                             validation_data=(X_test,y_test), shuffle = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model \n",
    "model.save('C:\\\\Users\\AUTOBOTS\\Downloads\\optical-character-recognition-OCR-master\\models\\model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('C:\\\\Users\\AUTOBOTS\\Downloads\\optical-character-recognition-OCR-master\\models/xtrain',X_train,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
