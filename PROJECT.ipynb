{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tkinter import filedialog\n",
    "from tkinter.filedialog import askopenfilename\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "import imutils\n",
    "import os.path\n",
    "import importlib\n",
    "from os import listdir\n",
    "from keras import backend as k\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from playsound import playsound\n",
    "import IPython\n",
    "from gtts import gTTS\n",
    "import pygame\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I entered train\n"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "print (\"I entered train\")\n",
    "# to get the name of the folder\n",
    "for name_folder in os.listdir(\"/home/anu/PROJECT/OCR-MODEL-master/Fnt\") :\n",
    "    name = '/home/anu/PROJECT/OCR-MODEL-master/Fnt/' + name_folder\n",
    "    for f in listdir(name):\n",
    "        # name of the folder is the name of the output\n",
    "        y_train.append(np.asarray(name_folder))\n",
    "y_train = np.asarray(y_train)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving the converted audio in a mp3 file named \n",
    "def image_segmentation(image_name):\n",
    "    #print( \"I entered letter segmentation\")\n",
    "    # reading the image\n",
    "    image = cv2.imread(image_name)\n",
    "\n",
    "    # converting the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # threshold to convert the image to pure black and white\n",
    "    thresh = cv2.threshold(gray, 0,255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "\n",
    "    # find the contours (continous blob of pixels ) in the image \n",
    "    contours = cv2.findContours(thresh,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Hack for compatibility with different OpenCV versions\n",
    "    contours = contours[0] \n",
    "    letter_image_regions = []\n",
    "\n",
    "    # now loop through each of the letter in the image \n",
    "    for contour in contours:\n",
    "        # get the rectangle that contains the contour\n",
    "        x,y,w,h = cv2.boundingRect(contour)\n",
    "        # compare the width and height of the contour to detect if it\n",
    "        # has one letter or not\n",
    "        if w/h >1.25:\n",
    "            # this is too wide for a single letter\n",
    "            continue\n",
    "        elif w<3 or h<3:\n",
    "            # this is a very small image probably a noise\n",
    "            continue\n",
    "        else:\n",
    "        # this is a normal letter by itself\n",
    "            letter_image_regions.append((x,y,w,h))\n",
    "\n",
    "    return letter_image_regions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the trained model\n",
      "loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"loading the trained model\")\n",
    "model = load_model('/home/anu/PROJECT/OCR-MODEL-master/model4.h5')\n",
    "print(\"loaded\")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_for_detection(raw_image):\n",
    "\n",
    "\t#remove tiny noises by blurring\n",
    "\tsm_image = cv2.GaussianBlur(raw_image,(5,5),0)\n",
    "\t\n",
    "\t#binarize\n",
    "\tret, bw_image = cv2.threshold(sm_image,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\t\n",
    "\t#dilate\n",
    "\tkernel = np.ones((2,2),np.uint8)\n",
    "\tbw_image = cv2.dilate(bw_image,kernel)\n",
    "\t\n",
    "\treturn bw_image\n",
    "\t\n",
    "def image_for_extraction(raw_image):\n",
    "\t\n",
    "\traw_image = cv2.GaussianBlur(raw_image,(3,3),0)\n",
    "\t\n",
    "\tret,no_sm_bw_image = cv2.threshold(raw_image,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "\t\n",
    "\treturn no_sm_bw_image\n",
    "\t\n",
    "def getTransformationMatrix(img):\n",
    "\t#input should be a binarized image - text white, bg black\n",
    "\t\n",
    "\t#Find all white pixels\n",
    "\tpts = np.empty([0,0])\n",
    "\tpts = cv2.findNonZero(img)\n",
    "\n",
    "\t#Get rotated rect of white pixels\n",
    "\trect = cv2.minAreaRect(pts)\n",
    "\t\n",
    "\t# rect[0] has the center of rectangle, rect[1] has width and height, rect[2] has the angle\n",
    "\t# To draw the rotated box and save the png image, uncomment below\n",
    "\tdrawrect = img.copy()\n",
    "\tdrawrect = cv2.cvtColor(drawrect, cv2.COLOR_GRAY2BGR)\n",
    "\tbox = cv2.boxPoints(rect)\n",
    "\tbox = np.int0(box) # box now has four vertices of rotated rectangle\n",
    "\tcv2.drawContours(drawrect,[box],0,(0,0,255),10)\n",
    "\t#v2.imwrite('rotated_rect.png', drawrect)\n",
    "\n",
    "\t#Change rotation angle if the tilt is in another direction\n",
    "\trect = list(rect)\n",
    "\tif (rect[1][0] < rect[1][1]): # rect.size.width > rect.size.height\n",
    "\t\ttemp = list(rect[1])\n",
    "\t\ttemp[0], temp[1] = temp[1], temp[0]\n",
    "\t\trect[1] = tuple(temp)\n",
    "\t\trect[2] = rect[2] + 90.0\n",
    "\n",
    "\t#convert rect back to numpy/tuple\n",
    "\trect = np.asarray(rect)\n",
    "\t\n",
    "\t#Rotate the image according to the found angle\n",
    "\trotated_image = np.empty([0,0])\n",
    "\tM = cv2.getRotationMatrix2D(rect[0], rect[2], 1.0)\n",
    "\t#img = cv2.warpAffine(img, M, (img.shape[1],img.shape[0]))\n",
    "\n",
    "\t#returns the transformation matrix for this rotation\n",
    "\treturn M\n",
    "\n",
    "def rotate(image, M):\n",
    "\treturn cv2.warpAffine(image, M, (image.shape[1],image.shape[0]))\n",
    "\n",
    "def findLines(bw_image, LinesThres):\n",
    "\t# making horizontal projection\n",
    "\thorProj = cv2.reduce(bw_image, 100, cv2.REDUCE_AVG)\n",
    "\n",
    "\t# make hist - same dimension as horProj - if 0 (space), then True, else False\n",
    "\tth = 0; # black pixels threshold value. this represents the space lines\n",
    "\thist = horProj <= th;\n",
    "\n",
    "\t#Get mean coordinate of white white pixels groups\n",
    "\tycoords = []\n",
    "\ty = 0\n",
    "\tcount = 0\n",
    "\tisSpace = False\n",
    "\n",
    "\tfor i in range(0, bw_image.shape[0]):\n",
    "\t\t\n",
    "\t\tif (not isSpace):\n",
    "\t\t\tif (hist[i]): #if space is detected, get the first starting y-coordinates and start count at 1\n",
    "\t\t\t\tisSpace = True\n",
    "\t\t\t\tcount = 1\n",
    "\t\t\t\ty = i\n",
    "\t\telse:\n",
    "\t\t\tif (not hist[i]):\n",
    "\t\t\t\tisSpace = False\n",
    "\t\t\t\t#when smoothing, thin letters will breakdown, creating a new blank lines or pixel rows, but the count will be small, so we set a threshold.\n",
    "\t\t\t\tif (count >=LinesThres):\n",
    "\t\t\t\t\tycoords.append(y // count)\n",
    "\t\t\telse:\n",
    "\t\t\t\ty = y + i\n",
    "\t\t\t\tcount = count + 1\n",
    "\n",
    "\tycoords.append(y // count)\n",
    "\t\n",
    "\t#returns y-coordinates of the lines found\n",
    "\treturn ycoords\n",
    "\n",
    "def LinesMedian(bw_image):\n",
    "\t# making horizontal projections\n",
    "\t\n",
    "\thorProj = cv2.reduce(bw_image, 1, cv2.REDUCE_AVG)\n",
    "\n",
    "\t# make hist - same dimension as horProj - if 0 (space), then True, else False\n",
    "\tth = 0; # black pixels threshold value. this represents the space lines\n",
    "\thist = horProj <= th;\n",
    "\n",
    "\t#Get mean coordinate of white white pixels groups\n",
    "\tycoords = []\n",
    "\ty = 0\n",
    "\tcount = 0\n",
    "\tisSpace = False\n",
    "\tmedian_count = []\n",
    "\n",
    "\tfor i in range(0, bw_image.shape[0]):\n",
    "\t\t\n",
    "\t\tif (not isSpace):\n",
    "\t\t\tif (hist[i]): #if space is detected, get the first starting y-coordinates and start count at 1\n",
    "\t\t\t\tisSpace = True\n",
    "\t\t\t\tcount = 1\n",
    "\t\t\t\t#y = i\n",
    "\t\telse:\n",
    "\t\t\tif (not hist[i]):\n",
    "\t\t\t\tisSpace = False\n",
    "\t\t\t\tmedian_count.append(count)\n",
    "\t\t\telse:\n",
    "\t\t\t\t#y = y + i\n",
    "\t\t\t\tcount = count + 1\n",
    "\tmedian_count.append(count)\n",
    "\t#ycoords.append(y / count)\n",
    "\t\n",
    "\t#returns counts of each blank rows of each of the lines found\n",
    "\treturn median_count\n",
    "\n",
    "def get_lines_threshold(percent, img_for_det):\n",
    "\tThresPercent = percent\n",
    "\tLinMed = LinesMedian(img_for_det)\n",
    "\tLinMed = sorted(LinMed)\n",
    "\tLinesThres = LinMed[len(LinMed)//3]*(ThresPercent//100.0)\n",
    "\tLinesThres = int(LinesThres)\n",
    "\treturn LinesThres\n",
    "\n",
    "def findSpaces(line, thres_space):\n",
    "\t\n",
    "\t# making vertical projections\n",
    "\t\n",
    "\tverProj = cv2.reduce(line, 0, cv2.REDUCE_AVG)\n",
    "\t#print('v',verProj)\n",
    "\t# make hist - same dimension as horProj - if 0 (space), then True, else False\n",
    "\tth = 0 # black pixels threshold value. this represents the space lines\n",
    "\thist = (verProj <= th)\n",
    "\t#print('hist',hist)\n",
    "\t#Get mean coordinate of white white pixels groups\n",
    "\txcoords = []\n",
    "\tx = 0\n",
    "\tcount = 0\n",
    "\tisSpace = False\n",
    "\t#print('shape',line.shape[1])\n",
    "\tfor i in range(0, line.shape[1]):\n",
    "\t\tif (not isSpace):\n",
    "\t\t\tif (hist[0][i]): #if space is detected, get the first starting x-coordinates and start count at 1\n",
    "\t\t\t\t#print('hist i',hist[0][i],i)\n",
    "\t\t\t\tisSpace = True\n",
    "\t\t\t\tcount = 1\n",
    "\t\t\t\tx = i\n",
    "\t\telse:\n",
    "\t\t\tif (not hist[0][i]):\n",
    "\t\t\t\tisSpace = False\n",
    "\t\t\t\t#when smoothing, thin letters will breakdown, creating a new blank lines or pixel columns, but the count will be small, so we set a threshold.\n",
    "\t\t\t\t#print count,\"\\t\",\n",
    "\t\t\t\tif (count > thres_space):\n",
    "\t\t\t\t\txcoords.append(x // count)\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = x + i\n",
    "\t\t\t\tcount = count + 1\n",
    "\t\n",
    "\n",
    "\txcoords.append(x // count)\n",
    "\t\n",
    "\treturn xcoords\n",
    "\n",
    "def SpacesMedian(line):\n",
    "\t\n",
    "\t# making vertical projections\n",
    "\t\n",
    "\tverProj = cv2.reduce(line, 0, cv2.REDUCE_AVG)\n",
    "\n",
    "\t# make hist - same dimension as horProj - if 0 (space), then True, else False\n",
    "\tth = 0; # black pixels threshold value. this represents the space lines\n",
    "\thist = verProj <= th;\n",
    "\n",
    "\t#Get mean coordinate of white white pixels groups\n",
    "\txcoords = []\n",
    "\tx = 0\n",
    "\tcount = 0\n",
    "\tisSpace = False\n",
    "\tmedian_count = []\n",
    "\tfor i in range(0, line.shape[1]):\n",
    "\t\tif (not isSpace):\n",
    "\t\t\tif (hist[0][i]): #if space is detected, get the first starting x-coordinates and start count at 1\n",
    "\t\t\t\tisSpace = True\n",
    "\t\t\t\tcount = 1\n",
    "\t\t\t\t#x = i\n",
    "\t\telse:\n",
    "\t\t\tif (not hist[0][i]):\n",
    "\t\t\t\tisSpace = False\n",
    "\t\t\t\t#when smoothing, thin letters will breakdown, creating a new blank lines or pixel columns, but the count will be small, so we set a threshold.\n",
    "\t\t\t\t#print count,\"\\t\",\n",
    "\t\t\t\t\n",
    "\t\t\t\t#append each count of rows of blank gaps found\n",
    "\t\t\t\tmedian_count.append(count)\n",
    "\t\t\t\t\n",
    "\t\t\t\t#if (count > 15):\n",
    "\t\t\t\t\t#xcoords.append(x / count)\n",
    "\t\t\telse:\n",
    "\t\t\t\t#x = x + i\n",
    "\t\t\t\tcount = count + 1\n",
    "\t\n",
    "\tmedian_count.append(count)\n",
    "\txcoords.append(x // count)\n",
    "\t\n",
    "\t#returns x-coordinates of the spaces found in the line\n",
    "\treturn median_count\n",
    "\t\n",
    "def get_spaces_threshold(ycoords, img_for_det) :\n",
    "\n",
    "\t## Find Median for setting threshold\n",
    "\tmedianList = []\n",
    "\tfor i in range ( 0, len(ycoords)-1 ):\n",
    "\t\tline = img_for_det[range(ycoords[i],ycoords[i+1])]\n",
    "\t\tmedianList.append(SpacesMedian(line))\n",
    "\t\n",
    "\t#medianList contains count of each blank columns found in all lines\n",
    "\t#including spaces found between each characters too\n",
    "\t\n",
    "\t#find the row among medianList[] with maximum length\n",
    "\tmax_len = len(medianList[0])\n",
    "\tmax_in = 0 #for index number\n",
    "\tfor i in range (0, len(medianList)):\n",
    "\t\tif max_len < len(medianList[i]):\n",
    "\t\t\tmax_len = len(medianList[i])\n",
    "\t\t\tmax_in = i\n",
    "\n",
    "\t#sort the row  having the maximum no. of elements (decending order)\n",
    "\tmList = sorted(medianList[max_in],reverse=True)\n",
    "\t\n",
    "\t#delete elements produced from the page's margin\n",
    "\tmList = np.delete(mList, [0,1,2])\n",
    "\t\n",
    "\tfirstItem = mList[0]\n",
    "\tfor i in range (len(mList)-1, 0, -1):\n",
    "\t\tif mList[i] < firstItem//2:\n",
    "\t\t\tmList = np.delete(mList,i)\n",
    "\n",
    "\tmean = np.mean(mList)\n",
    "\tthreshold_space = mean/2\n",
    "\t\n",
    "\treturn threshold_space\n",
    "\n",
    "def get_words(raw_image):\n",
    "\t\n",
    "\t#Returns a list/array of all the words found along with the number of words on each line.\n",
    "\t\n",
    "\t#preprocessing of the image\n",
    "\t\n",
    "\t#img_for_det used for detecting the character and lines boundaries\n",
    "\timg_for_det = image_for_detection(raw_image)\n",
    "\t\n",
    "\t#img_for_ext used for the actual extraction of the characters\n",
    "\timg_for_ext = image_for_extraction(raw_image)\n",
    "\t\n",
    "\t#get the rotated angle of the tilt\n",
    "\tM = getTransformationMatrix(img_for_det) # M is transformation matrix\n",
    "\t#rotate the iamge with M\n",
    "\timg_for_det = rotate(img_for_det,M)\n",
    "\t#rotate image that will be used for extraction too\n",
    "\timg_for_ext = rotate(img_for_ext,M)\n",
    "\t\n",
    "\t#for debugging purpose, we also write the images to files\n",
    "\t#cv2.imwrite('img_for_detection.png', img_for_det)\n",
    "\t#cv2.imwrite('img_for_extraction.png', img_for_ext)\n",
    "\n",
    "\t#get threshold to determine how much gap should be considered as the line gap\n",
    "\tLinesThres = get_lines_threshold(50, img_for_det)\n",
    "\tycoords = findLines(img_for_det, LinesThres)\n",
    "\t#print(ycoords)\n",
    "\t# save image with lines printed ==========\n",
    "\timg_with_lines = img_for_ext.copy()\n",
    "\tfor i in ycoords:\n",
    "\t\tcv2.line(img_with_lines,(0,i),(img_with_lines.shape[1],i),255,1)\n",
    "\t#cv2.imwrite('img_with_lines.png', img_with_lines)\n",
    "\t#==========\n",
    "\n",
    "\t### =========== lines detection finish - ===========================\n",
    "\t\n",
    "\t#calculate max_line_height on each line\n",
    "\tmax_height_on_line = []\n",
    "\tfor i in range ( 0, len(ycoords)-1 ): #iterate line\n",
    "\t\t\n",
    "\t\tline = img_for_ext[range(ycoords[i],ycoords[i+1])]\n",
    "\n",
    "\t\t# to find max_line_height of each line we find contours again in this line only\n",
    "      \n",
    "\t\t#cnt_len = cv2.arcLength(cnt, True)\n",
    "\t\tcontour0 = cv2.findContours(line.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\t\tcontours = [cv2.approxPolyDP(cnt,2,True) for cnt in contour0[0]]\n",
    "\n",
    "        # === Extract Bounding Rectangles\n",
    "\t\tmaxArea = 0\n",
    "\t\trect=[]\n",
    "\t\tfor ctr in contours:\n",
    "\t\t\tmaxArea = max(maxArea,cv2.contourArea(ctr))\n",
    "\n",
    "\t\tareaRatio = 0.008\n",
    "\n",
    "\t\tfor ctr in contours:\n",
    "\t\t\tif cv2.contourArea(ctr) > maxArea * areaRatio: \n",
    "\t\t\t\trect.append(cv2.boundingRect(cv2.approxPolyDP(ctr,1,True)))\n",
    "\t\t\n",
    "\t\t#Find max_line_height and width\n",
    "\t\tmax_line_height = 0\n",
    "\t\t\n",
    "\t\t#for x in rect:\n",
    "\t\t#\t\tround(x)\n",
    "\t\t#rect.append(x)\n",
    "\t\tfor i in rect:\n",
    "\t\t\tx = i[0]\n",
    "\t\t\ty = i[1]\n",
    "\t\t\tw = i[2]\n",
    "\t\t\th = i[3]\n",
    "\t\t\t\n",
    "\t\t\tif(h>max_line_height):\n",
    "\t\t\t\tmax_line_height = h\n",
    "\t\t\n",
    "\t\tmax_height_on_line.append(max_line_height)\n",
    "\t\n",
    "\t### =========== space in a line detection begins ===================\n",
    "\n",
    "\t#get the threshold to determine how much gap should be considered as the space between the words\n",
    "\tthreshold_space = get_spaces_threshold(ycoords, img_for_det)\n",
    "\n",
    "\t#split lines based on the ycoords of the detected lines\n",
    "\t#each line is put into the var 'line' and the words are found\n",
    "\t#based on the threshold_space value.\n",
    "\n",
    "\twords_on_line=[]\n",
    "\tall_words=[]\n",
    "\tcount = 0\n",
    "\tnumber_of_words = 0\n",
    "\n",
    "\tfor i in range ( 0, len(ycoords)-1 ): #iterate line\n",
    "\n",
    "\t\tline = img_for_det[range(ycoords[i],ycoords[i+1])]\n",
    "\t\t#cv2.imwrite(os.path.join(\"C:\\\\Users\\\\HP\\\\Desktop\\\\optical-character-recognition-OCR-master\\\\out\\\\\",'line_no'+str(i)+'.png', line))\n",
    "\t\t\n",
    "\t\t#finding the x-coordinates of the spaces\n",
    "\t\txcoords = findSpaces(line, threshold_space)\n",
    "\t\t\n",
    "\t\t#print((xcoords))\n",
    "\t\tfor x in xcoords:\n",
    "\t\t\tcv2.line(line, (x,0), (x,line.shape[0]), 255, 1)\n",
    "\t\t#cv2.imwrite('img/'+str(i)+'.png', line)\n",
    "\t\tcv2.imwrite(os.path.join(\"/home/anu/PROJECT/OCR-MODEL-master/extract/line/\" , str(i)+ \".png\"),~line)\n",
    "\t\tcount = 0\n",
    "\t\t\n",
    "\t\tfor j in range (0, len(xcoords)-1 ): #iterate words\n",
    "\t\t\t\n",
    "\t\t\t#use image with no smoothing\n",
    "\t\t\tline = img_for_ext[range(ycoords[i],ycoords[i+1])]\n",
    "\t\t\t#print('line',line)\n",
    "\t\t\tword = line[:, xcoords[j]: xcoords[j+1]]\n",
    "\t\t\t#print(word)\n",
    "\t\t\tall_words.append(word)\n",
    "\t\t\t#cv2.imwrite('img/words/'+str(number_of_words)+'.png', word)\n",
    "\t\t\tcv2.imwrite(os.path.join(\"/home/anu/PROJECT/OCR-MODEL-master/extract/word/\" , str(number_of_words)+ \".png\"),~word)\n",
    "\t\t\tcount = count + 1\n",
    "\t\t\tnumber_of_words = number_of_words + 1\n",
    "\t\t\t#Generate space here\n",
    "\t\t\n",
    "\t\twords_on_line.append(count)\n",
    "\t\t# Line Change\n",
    "\t\n",
    "\treturn all_words, words_on_line, max_height_on_line\n",
    "\n",
    "def fix_i_j(rect, max_line_height, max_w):\n",
    "\t# ========== correct dots commas\n",
    "\tj = 0\n",
    "\ti_dot_list = []\n",
    "\n",
    "\tfor i in rect:\n",
    "\t\tx = i[0]\n",
    "\t\ty = i[1]\n",
    "\t\tw = i[2]\n",
    "\t\th = i[3]\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t#if the dot of i is the last element in the rect, the [j+1] index will not work. so we put [j-1] separately here\n",
    "\t\tif  (j is len(rect)-1 and (h < max_line_height/3)):\n",
    "\t\t\t\n",
    "\t\t\tif( (h < max_line_height/3) and (abs(rect[j-1][0]+rect[j-1][2] - (x+w)) < max_w/3.5) ):\n",
    "\t\t\t\t#correct i\n",
    "\t\t\t\t#rect[j-1][3] = rect[j-1][3] + (rect[j-1][1] - y)\n",
    "\t\t\t\trect[j-1] = (rect[j-1][0], rect[j-1][1], rect[j-1][2], rect[j-1][3] + (rect[j-1][1] - y))\n",
    "\t\t\t\t\n",
    "\t\t\t\t#rect[j-1][1] = y\n",
    "\t\t\t\trect[j-1] = (rect[j-1][0], y, rect[j-1][2], rect[j-1][3])\n",
    "\t\t\t\ti_dot_list.append(j)\n",
    "\t\t\t\n",
    "\t\t\telif (h < max_line_height/2.4 and y > (rect[j-1][1] + rect[j-1][3]/3)):\n",
    "\t\t\t\t#rect[j][1] = rect[j][1] - (max_line_height/2)\n",
    "\t\t\t\trect[j] = [x,y-(max_line_height/2),w,h+(max_line_height/2)]\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t\t#if the dot of i is not the last element in the rect\n",
    "\t\telse:\n",
    "\t\t\n",
    "\t\t\tif( (h < max_line_height/3) and (abs(rect[j+1][0]+rect[j+1][2] - (x+w)) < max_w/3.5)):\n",
    "\t\t\t\t#correct i\n",
    "\t\t\t\t#rect[j+1][3] = rect[j+1][3] + (rect[j+1][1] - y)\n",
    "\t\t\t\trect[j+1] = (rect[j+1][0], rect[j+1][1], rect[j+1][2], rect[j+1][3] + (rect[j+1][1] - y))\n",
    "\t\t\t\t\n",
    "\t\t\t\t#rect[j+1][1] = y\n",
    "\t\t\t\trect[j+1] = (rect[j+1][0], y, rect[j+1][2], rect[j+1][3])\n",
    "\t\t\t\t\n",
    "\t\t\t\ti_dot_list.append(j)\n",
    "\t\t\t\t\n",
    "\t\t\telif( (h < max_line_height/3) and (abs(rect[j-1][0]+rect[j-1][2] - (x+w)) < max_w/3.5) ):\n",
    "\t\t\t\t#correct i\n",
    "\t\t\t\t#rect[j-1][3] = rect[j-1][3] + (rect[j-1][1] - y)\n",
    "\t\t\t\trect[j-1] = (rect[j-1][0], rect[j-1][1], rect[j-1][2], rect[j-1][3] + (rect[j-1][1] - y))\n",
    "\t\t\t\t\n",
    "\t\t\t\t#rect[j-1][1] = y\n",
    "\t\t\t\trect[j-1] = (rect[j-1][0], y, rect[j-1][2], rect[j-1][3])\n",
    "\t\t\t\t\n",
    "\t\t\t\ti_dot_list.append(j)\n",
    "\t\t\t\n",
    "\t\t\telif (h < max_line_height/2.4 and y > (rect[j-1][1] + rect[j-1][3]/3)):\n",
    "\t\t\t\t#rect[j][1] = rect[j][1] - (max_line_height/2)\n",
    "\t\t\t\trect[j] = [x,y-(max_line_height/2),w,h+(max_line_height/2)]\n",
    "\t\t\t\t\n",
    "\t\tj = j + 1\n",
    "\t\t\t\n",
    "\t# ===== end of fixing dots in i and j\n",
    "\n",
    "\t#delete the dots from rect array which belongs to i and j\n",
    "\trect = np.delete(rect, i_dot_list, axis=0)\n",
    "\t# =======================\n",
    "\t\n",
    "\treturn rect\n",
    "\n",
    "def get_characters(raw_image,max_line_height,line,word):\n",
    "\n",
    "\t# === Find Contours\n",
    "\n",
    "\tmo_image = raw_image.copy()\n",
    "\tcontour0 = cv2.findContours(mo_image.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\tcontours = [cv2.approxPolyDP(cnt,2,True) for cnt in contour0[0]]\n",
    "\n",
    "\t# === Extract Bounding Rectangles\n",
    "\tmaxArea = 0\n",
    "\trect=[]\n",
    "\tfor ctr in contours:\n",
    "\t\tmaxArea = max(maxArea,cv2.contourArea(ctr))\n",
    "\n",
    "\tareaRatio = 0.008\n",
    "\n",
    "\tfor ctr in contours:\n",
    "\t\tif cv2.contourArea(ctr) > maxArea * areaRatio: \n",
    "\t\t\trect.append(cv2.boundingRect(cv2.approxPolyDP(ctr,1,True)))\n",
    "\t\t\t\n",
    "\t#Find max_line_height and width\n",
    "\tmax_w = 0\n",
    "\t\n",
    "\tfor i in rect:\n",
    "\t\tx = i[0]\n",
    "\t\ty = i[1]\n",
    "\t\tw = i[2]\n",
    "\t\th = i[3]\n",
    "\n",
    "\t\tif(w>max_w):\n",
    "\t\t\tmax_w = w\n",
    "\n",
    "\t# Sort rect left to right, top to bottom, plus correct the dots and commas\n",
    "\n",
    "\t#sort all rect by their x\n",
    "\trect.sort(key=lambda b: b[0])\n",
    "\n",
    "\t#There are two contours detected for the characters such as i and j. So we need to merge two contours of 'dot' and the 'base' of i and j\n",
    "\t#Fix i and j\n",
    "\trect = fix_i_j(rect, max_line_height, max_w)\n",
    "\t\n",
    "\t# remove artifacts - usually artifacts found are manipulated as 0 height by the i&j dot fixing functions\n",
    "\tminus_count = 0\n",
    "\tminus_list = []\n",
    "\tfor i in rect:\n",
    "\t\tx = i[0]\n",
    "\t\ty = i[1]\n",
    "\t\tw = i[2]\n",
    "\t\th = i[3]\n",
    "\t\t\n",
    "\t\tif h<0:\n",
    "\t\t\tminus_list.append(minus_count)\n",
    "\t\t\n",
    "\t\tminus_count = minus_count + 1\n",
    "\t\n",
    "\trect = np.delete(rect, minus_list, axis=0)\n",
    "\t# ================= end ===============================\n",
    "\t\n",
    "\trect_segmented_image = mo_image.copy()\n",
    "\t\n",
    "\tsymbols=[]\n",
    "\n",
    "\tall_letters = []\n",
    "\n",
    "\t#count used for filename naming\n",
    "\tcount = 0\n",
    "\n",
    "\t#raw_input('>')\n",
    "\tfor i in rect:\n",
    "\t\tx = i[0]\n",
    "\t\ty = i[1]\n",
    "\t\tw = i[2]\n",
    "\t\th = i[3]\n",
    "\t\t\n",
    "\t\tp1 = (x,y)\n",
    "\t\tp2 = (x+w,y+h)\n",
    "\t\t\n",
    "\t\tletter = mo_image[y:y+h,x:x+w]\n",
    "\t\t\n",
    "\t\t#resize letter image to 32x32 ======================================\n",
    "\t\t#resize letter content to 28x28\n",
    "\t\t\n",
    "\t\to_height = letter.shape[0]\n",
    "\t\to_width = letter.shape[1]\n",
    "\t\t\n",
    "\t\t#if errors occurs due to the unwanted artifacts, then the height will somehow become zero.\n",
    "\t\tif (o_height == 0):\n",
    "\t\t\tletter = np.zeros((30, 30, 1), np.uint8)\n",
    "\t\t\to_height = letter.shape[0]\n",
    "\t\t\to_width = letter.shape[1]\n",
    "\t\t\n",
    "\t\t#resize height to 28 pixels\n",
    "\t\t#we need three different conditions to work well with the aspect ratios\n",
    "\t\t\n",
    "\t\tif(o_height>o_width): # height greater than width\n",
    "\n",
    "\t\t\taspectRatio = o_width / (o_height*1.0)\n",
    "\t\t\t\n",
    "\t\t\theight = 26\n",
    "\t\t\twidth = int(height * aspectRatio)\n",
    "\t\t\tletter = cv2.resize(letter, (width,height))\n",
    "\t\t\t\n",
    "\t\t\t#add border which results adding of padding\n",
    "\t\t\t\n",
    "\t\t\tremaining_pixels_w = abs(32 - letter.shape[1])\n",
    "\t\t\tadd_left = remaining_pixels_w // 2\n",
    "\t\t\tadd_right = remaining_pixels_w - add_left\n",
    "\t\t\tletter = cv2.copyMakeBorder(letter, 0, 0, add_left, add_right, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "\t\t\t\n",
    "\t\t\tremaining_pixels_h = abs(32 - letter.shape[0])\n",
    "\t\t\tadd_top = remaining_pixels_h // 2\n",
    "\t\t\tadd_bottom = remaining_pixels_h - add_top\n",
    "\t\t\tletter = cv2.copyMakeBorder(letter, add_top, add_bottom, 0, 0, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "\t\t\t\n",
    "\t\t\t# =================\n",
    "\t\t\t\n",
    "\t\telif(o_width>o_height): # width greater than height\n",
    "\t\t\t\n",
    "\t\t\taspectRatio = abs(o_height // (o_width*1))\n",
    "\t\t\t\n",
    "\t\t\twidth = 26\n",
    "\t\t\theight = int(width * aspectRatio)\n",
    "\t\t\t\n",
    "\t\t\tletter = cv2.resize(letter, (width,height))\n",
    "\t\t\t\n",
    "\t\t\t#add border which results adding of padding\n",
    "\t\t\tremaining_pixels_w = abs(32 - letter.shape[1])\n",
    "\t\t\tadd_left = remaining_pixels_w / 2\n",
    "\t\t\tadd_right = remaining_pixels_w - add_left\n",
    "\t\t\tletter = cv2.copyMakeBorder(letter, 0, 0, add_left, add_right, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "\t\t\t\n",
    "\t\t\tremaining_pixels_h = abs(32 - letter.shape[0])\n",
    "\t\t\tadd_top = remaining_pixels_h / 2\n",
    "\t\t\tadd_bottom = remaining_pixels_h - add_top\n",
    "\t\t\tletter = cv2.copyMakeBorder(letter, add_top, add_bottom, 0, 0, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "\t\t\t\n",
    "\t\t\t# =================\n",
    "\t\t\n",
    "\t\telse: # both height and width equal\n",
    "\t\t\tletter = cv2.resize(letter, (26,26))\n",
    "\t\t\t\n",
    "\t\t\t#add border which results adding of padding\n",
    "\t\t\tremaining_pixels_w = abs(32 - letter.shape[1])\n",
    "\t\t\tadd_left = remaining_pixels_w / 2\n",
    "\t\t\tadd_right = remaining_pixels_w - add_left\n",
    "\t\t\tletter = cv2.copyMakeBorder(letter, 0, 0, add_left, add_right, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "\t\t\t\n",
    "\t\t\tremaining_pixels_h = abs(32 - letter.shape[0])\n",
    "\t\t\tadd_top = remaining_pixels_h / 2\n",
    "\t\t\tadd_bottom = remaining_pixels_h - add_top\n",
    "\t\t\tletter = cv2.copyMakeBorder(letter, add_top, add_bottom, 0, 0, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "\t\t\tprint('letter',letter)\n",
    "\t\t\t# =================\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tcv2.imwrite(os.path.join(\"/home/anu/PROJECT/OCR-MODEL-master/extract/\",\"line\"+str(line)+\".png\"+\"wo_\"+str(word)+\".png\"+\"ch_\"+str(count)+\".png\"),letter)\n",
    "\t\tcount = count + 1\n",
    "\t\t\n",
    "\t\tletter = letter / 255.0\n",
    "\t\t\n",
    "\t\tletter = np.reshape(letter,(1024,1))\n",
    "\t\t\n",
    "\t\tall_letters.append(letter)\n",
    "\n",
    "\t\t#=================================\n",
    "\n",
    "\t\tcv2.rectangle(rect_segmented_image,p1,p2,255,1)\n",
    "\n",
    "\tcv2.imwrite('segmented.png', rect_segmented_image)\n",
    "\n",
    "\treturn all_letters\n",
    "\n",
    "def image_segmentation1(name1):\n",
    "    raw_image = cv2.imread(name1,0)\n",
    "    count=0\n",
    "        #get all the words (as an numpy image array), words on each line, and maximum height on that line\n",
    "    all_words, words_on_line, max_height_on_line = get_words(raw_image)\n",
    "    #for i in range(0, len(words_on_line)):\n",
    "    #    for j in range(0, words_on_line[i]):\n",
    "    #        all_characters = get_characters(all_words[count],max_height_on_line[i],i,j)\n",
    "    #        cv2.imshow(\"all_words[count]\",all_words[count])\n",
    "    #        count = count + 1\n",
    "    print (\"no. of lines = \",len(words_on_line))\n",
    "    print (words_on_line)\n",
    "    print (\"no. of words = \",len(all_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big_merged.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    #\"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    #\"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    #\"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    #\"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    #\"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "    \n",
    "def edits2(word): \n",
    "    #\"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to resize the image into appropriate dimensions\n",
    "def resize(img):\n",
    "    img = cv2.resize(img,(20,20))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Openvoice():\n",
    "    import os \n",
    "\n",
    "    # The text that you want to convert to audio \n",
    "    #file_name='/home/anu/PROJECT/OCR-MODEL-master/extract/letter/'+ text_file\n",
    "    #with open(file_name,'r') as myfile:\n",
    "    #    data=myfile.read().replace('\\n','')\n",
    "    str1 = ' '.join(words)\n",
    "    mytext=str1\n",
    "\n",
    "    print(mytext)\n",
    "    # Language in which you want to convert \n",
    "    language = 'en'\n",
    " \n",
    "    myobj = gTTS(text=mytext, lang=language, slow=False) \n",
    "    \n",
    "    import time\n",
    "    #timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    myobj.save(\"/home/anu/PROJECT/OCR-MODEL-master/extract/output/\"+ timestr+'.mp3') \n",
    "\n",
    "    #print timestr\n",
    "    #pygame.mixer.init()\n",
    "    #pygame.mixer.music.load('/home/anu/PROJECT/OCR-MODEL-master/extract/output/' + timestr +'.mp3')\n",
    "    #pygame.mixer.music.play()\n",
    "\n",
    "    # Saving the converted audio in a mp3 file named \n",
    "    # welcome \n",
    "    # Playing the converted file \n",
    "    #os.system(\"welcome.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():    \n",
    "    listd=[]\n",
    "    new=[]\n",
    "    global words\n",
    "    global ans\n",
    "\n",
    "    words=[]\n",
    "    path='/home/anu/PROJECT/OCR-MODEL-master/extract/word/'\n",
    "    for image in listdir(path):\n",
    "        listd.append(image)\n",
    "    res = [int(sub.split('.')[0]) for sub in listd] \n",
    "    res.sort()\n",
    "\n",
    "    for image_name in res:\n",
    "        a=str(image_name)+'.png'\n",
    "        new.append(a)\n",
    "    #print(new)\n",
    "\n",
    "\n",
    "    for image_name in new:\n",
    "        counter = 1\n",
    "        #print(image_name)\n",
    "        # constructing the name of the file \n",
    "        file_name = '/home/anu/PROJECT/OCR-MODEL-master/extract/word/' + image_name\n",
    "\n",
    "        # getting segmented images \n",
    "        letters_in_image = image_segmentation(file_name)\n",
    "\n",
    "        # sorting the letters so that letters that appear before is addressed first \n",
    "        letters_in_image = sorted(letters_in_image, key=lambda x: x[0])\n",
    "\n",
    "        ans = \"\"\n",
    "        ans1=\"\"\n",
    "        for (x,y,w,h) in letters_in_image:\n",
    "            image = cv2.imread(file_name,0)\n",
    "            letter = image[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "\n",
    "            #cv2.imwrite(str(counter)+'.jpg', letter)\n",
    "            counter = counter + 1\n",
    "\n",
    "            letter  = resize(letter)/255\n",
    "            X_test = np.asarray(letter)\n",
    "            X_test = np.reshape(X_test, [-1,20,20,1])\n",
    "            output = [np.argmax(model.predict(X_test, verbose = 0))]\n",
    "            output = label_encoder.inverse_transform(output)\n",
    "            ans1+=output[0].lower()\n",
    "            ans=correction(ans1)\n",
    "        words.append(ans)\n",
    "        PathTextBox2.insert(END, \" \" + ans)\n",
    "    #print(\" \", ans)\n",
    "    print(words)\n",
    "    \n",
    "\n",
    "    path='/home/anu/PROJECT/OCR-MODEL-master/extract/letter/'\n",
    "    for i in words:\n",
    "        global text_file\n",
    "        import time\n",
    "        #timestr1 = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        text_file = open(path+timestr+'.txt', \"a\")\n",
    "        text_file.write(i)\n",
    "        text_file.write(' ')\n",
    "        text_file.close()\n",
    "    Openvoice()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def stop():\n",
    "    pygame.mixer.music.pause()\n",
    "    \n",
    "def play():\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load('/home/anu/PROJECT/OCR-MODEL-master/extract/output/' + timestr +'.mp3')\n",
    "    pygame.mixer.music.play()\n",
    "        \n",
    "def Image():\n",
    "    img=cv2.imread(name1)\n",
    "    cv2.imshow(\"Input Image\",img)\n",
    "\n",
    "def clear():\n",
    "    PathTextBox2.delete(\"1.0\",END)\n",
    "    PathTextBox.delete(\"1.0\",END)\n",
    "    for root, dirs, files in os.walk('/home/anu/PROJECT/OCR-MODEL-master/extract/word/'):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "        for d in dirs:\n",
    "            shutil.rmtree(os.path.join(root, d))\n",
    "\n",
    "    for root, dirs, files in os.walk('/home/anu/PROJECT/OCR-MODEL-master/extract/line/'):\n",
    "        for f in files:\n",
    "            os.unlink(os.path.join(root, f))\n",
    "        for d in dirs:\n",
    "            shutil.rmtree(os.path.join(root, d))\n",
    "    \n",
    "def OpenTxt():\n",
    "    filename='/home/anu/PROJECT/OCR-MODEL-master/extract/letter/'+timestr+'.txt'\n",
    "\n",
    "    #from subprocess import call\n",
    "    #call(\"/home/anu/PROJECT/OCR-MODEL-master/extract/letter/20200427-201115.txt\")\n",
    "    try:\n",
    "        os.system( '{0} \"{1}\"'.format( os.environ['EDITOR'], filename ))\n",
    "    except:\n",
    "         os.system( 'gedit \"{0}\"'.format(filename) )\n",
    "    \n",
    "\n",
    "def OpenTxt_B():\n",
    "    filename='/home/anu/PROJECT/OCR-MODEL-master/extract/translation/'+timestr+'.txt'\n",
    "\n",
    "    #from subprocess import call\n",
    "    #call(\"/home/anu/PROJECT/OCR-MODEL-master/extract/letter/20200427-201115.txt\")\n",
    "    try:\n",
    "        os.system( '{0} \"{1}\"'.format( os.environ['EDITOR'], filename ))\n",
    "    except:\n",
    "         os.system( 'gedit \"{0}\"'.format(filename) )    \n",
    "    \n",
    "def translate():\n",
    "    import re\n",
    "    trans=[]\n",
    "    braille = '⠁⠃⠉⠙⠑⠋⠛⠓⠊⠚⠅⠇⠍⠝⠕⠏⠟⠗⠎⠞⠥⠧⠺⠭⠽⠵' '⠁⠃⠉⠙⠑⠋⠛⠓⠊⠚'\n",
    "    normal  = 'abcdefghijklmnopqrstuvwxyz' '1234567890'\n",
    "    table = str.maketrans(normal, braille) # define the translation table\n",
    "\n",
    "    def make_braille(data):\n",
    "        data = re.sub(r'(\\d+)', r'⠼\\1', data) # add ⠼ before every number\n",
    "        data = data.translate(table) # convert the remaining characters to braille\n",
    "        return data\n",
    "    print(\"In translate\")\n",
    "    \n",
    "    for i in words:\n",
    "        temp=make_braille(i)\n",
    "        \n",
    "        trans.append(temp)\n",
    "    path='/home/anu/PROJECT/OCR-MODEL-master/extract/translation/'\n",
    "    for i in trans:\n",
    "        global text_file\n",
    "        import time\n",
    "        #timestr1 = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        text_file = open(path+timestr+'.txt', \"a\")\n",
    "        text_file.write(i)\n",
    "        text_file.write(' ')\n",
    "        text_file.close()  \n",
    "    \n",
    "def OpenFile():\n",
    "    global letter_images\n",
    "    global name1\n",
    "    name1 = askopenfilename(initialdir=\"/home/anu/PROJECT/OCR-MODEL-master\",\n",
    "                           filetypes =((\"PNG File\", \"*.png\"),(\"BMP File\", \"*.bmp\"),(\"JPG File\", \"*.jpg\"),(\"JPEG File\", \"*.jpeg\")),\n",
    "                           title = \"Choose a file.\"\n",
    "                           ) \n",
    "    image_segmentation1(name1)\n",
    "    PathTextBox.delete(\"1.0\",END)\n",
    "    PathTextBox.insert(END,name1)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of lines =  4\n",
      "[9, 9, 6, 6]\n",
      "no. of words =  30\n",
      "['he', 'lax', 'on', 'his', 'your', 'lire', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'bro', 'bell', 'slightly', 'domed', 'and', 'died', 'by', 'arches', 'into', 'stiff', 'sections']\n",
      "he lax on his your lire back and if he lifted his head a little he could see his bro bell slightly domed and died by arches into stiff sections\n",
      "In translate\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageTk,Image\n",
    "from tkinter import ttk\n",
    "from ttkthemes import ThemedStyle\n",
    "\n",
    "\n",
    "root = Tk(  )\n",
    "root.resizable(0,0)      #will disable max/min tab of window\n",
    "root.configure(bg=\"#808080\")\n",
    "location=str()\n",
    "#root.geometry(\"3000X2000\")\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "im = Image.open('ocrm.png')\n",
    "ph = ImageTk.PhotoImage(im)\n",
    "#plt.imshow(ph)\n",
    "\n",
    "Title = root.title( \"Optical Character Recognition Model\")\n",
    "path = StringVar()\n",
    "\n",
    "#s = ttk.Style()\n",
    "#s.configure('tab1.TFrame', background='#BB8FCE', foreground='blue')\n",
    "home= \" Hi ! Welcome to OPTICAL CHARACTER RECOGNITION MODEL \"\n",
    "about= \" OCR model is conversion of images of typed \\n or printed text into machine-encoded text, \\n whether from a scanned document, a photo of a document, \\n a scene-photo (for example the text on signs and \\n billboards in a landscape photo). \\n OCR is a field of research in pattern recognition, \\n artificial intelligence and computer vision.\"\n",
    "contact= \"A. P. SHAH INSTITUTE OF TECHNOLOGY, GHODBUNDER ROAD,\\n\\n\\t\\tTHANE (WEST)\"\n",
    "#ADING TABS\n",
    "TAB_CONTROL = ttk.Notebook(root)\n",
    "\n",
    "\n",
    "style = ttk.Style()\n",
    "style = ThemedStyle(TAB_CONTROL)\n",
    "style.set_theme(\"ubuntu\")\n",
    "\n",
    "style.configure(\"TNotebook.Tab\", background=\"#6B6A6A\", foreground=\"#6C7907\", borderwidth=10)\n",
    "style.configure('tab1.TFrame',expand=\"full\", foreground='blue')\n",
    "\n",
    "style.configure('TButton', background=\"#8EA002\", foreground=\"black\")\n",
    "style.configure('TLabel', foreground=\"black\")\n",
    "\n",
    "TAB1 = ttk.Frame(TAB_CONTROL, style = 'tab1.TFrame')\n",
    "TAB_CONTROL.add(TAB1, text= 'HOME')\n",
    "\n",
    "TAB2 = ttk.Frame(TAB_CONTROL, style = 'tab1.TFrame')\n",
    "TAB_CONTROL.add(TAB2, text='INPUT WINDOW')\n",
    "\n",
    "TAB3 = ttk.Frame(TAB_CONTROL, style = 'tab1.TFrame')\n",
    "TAB_CONTROL.add(TAB3, text='OUTPUT WINDOW')\n",
    "\n",
    "TAB4 = ttk.Frame(TAB_CONTROL, style = 'tab1.TFrame')\n",
    "TAB_CONTROL.add(TAB4, text='ABOUT OCR MODEL')\n",
    "\n",
    "TAB5 = ttk.Frame(TAB_CONTROL, style = 'tab1.TFrame')\n",
    "TAB_CONTROL.add(TAB5, text='CONTACT US')\n",
    "\n",
    "TAB_CONTROL.pack(expand=1, fill=\"both\")\n",
    "\n",
    "ttk.Label(TAB4, text= about, style=\"TLabel\", foreground=\"#3C4C00\").pack(padx=5, pady=20)\n",
    "ttk.Label(TAB5, text= contact, style=\"TLabel\",foreground=\"#3C4C00\").pack(padx=30, pady=50)\n",
    "\n",
    "\n",
    "#tkinter.lab=Label(TAB1,image=ph).place(x=100,y=70)\n",
    "\n",
    "#TAB_CONTROL.add(TAB1, text=\"profile\", image=ph)#, compound=tk.TOP) # use the tk constants\n",
    "try:\n",
    "    lab=Label(TAB1,image=ph).place(x=60,y=45)\n",
    "except TclError:\n",
    "    ttk.Label(TAB5, text= home, style=\"TLabel\").pack(anchor=CENTER,padx=30, pady=50)\n",
    "\n",
    "#creating another frames\n",
    "BTAB3 = ttk.Frame(TAB3, style = 'tab1.TFrame')\n",
    "BTAB3.pack(side=BOTTOM, expand=1,pady=10)\n",
    "TTAB3= ttk.Frame(TAB3, style = 'tab1.TFrame')\n",
    "TTAB3.pack(side=TOP, expand=1)\n",
    "CTAB3= ttk.Frame(TAB3, style = 'tab1.TFrame')\n",
    "CTAB3.pack(side=TOP, expand=1)\n",
    "\n",
    "InputLabel = ttk.Label(TAB2,text = \"INPUT IMAGE\",style=\"TLabel\")\n",
    "InputLabel.pack(side= TOP, pady=15)\n",
    "BrowseButton = ttk.Button(TAB2,text=\"Browse\",style=\"TButton\",command = OpenFile)\n",
    "BrowseButton.pack(side = TOP, pady=10,padx=10)\n",
    "\n",
    "PathTextBox = Text(TAB2,width = 60,height= 2,bg=\"#EEF0C3\", fg=\"black\")\n",
    "PathTextBox.pack(side = TOP, fill = X,padx=10)\n",
    "SubmitButton = ttk.Button(TAB2,text=\"Submit\",style=\"TButton\",command = predict)\n",
    "SubmitButton.pack(side = TOP, padx=10, pady =15) \n",
    "ClearButton = ttk.Button(TAB2,text=\"Start Over\",style=\"TButton\",command = clear)\n",
    "ClearButton.pack(side = TOP, pady=20,padx=10)\n",
    "\n",
    "PathLabel = ttk.Label(TTAB3,text = \"Text \",style=\"TLabel\")\n",
    "PathLabel.pack(pady=7)\n",
    "PathTextBox2 = Text(TTAB3,width = 60, height=3,bg=\"#EEF0C3\", fg=\"black\")\n",
    "PathTextBox2.pack(side = TOP, fill = X,pady=10)\n",
    "                \n",
    "TxtLabel = ttk.Label(BTAB3,text = \"Open Image             English TextFile\",style=\"TLabel\")\n",
    "TxtLabel.pack(pady=10)\n",
    "ReadButton = ttk.Button(BTAB3, style=\"TButton\",text=\"Image\",command = Image )#Opens location where voice file is being stored\n",
    "ReadButton.pack(padx=50, side=LEFT)\n",
    "TxtButton = ttk.Button(BTAB3,text=\"Open\",style=\"TButton\",command = OpenTxt )#Opens location where voice file is being stored\n",
    "TxtButton.pack(padx=50,side=LEFT)\n",
    "\n",
    "PathLabel = ttk.Label(CTAB3,text = \"Convert to Braille: \",style=\"TLabel\")\n",
    "PathLabel.pack(pady=10)\n",
    "TButton = ttk.Button(CTAB3,text=\"Translate\",style=\"TButton\",command = translate )#Opens location where voice file is being stored\n",
    "TButton.pack(padx=50,side=LEFT)\n",
    "TxtButton = ttk.Button(CTAB3,text=\"Open File\",style=\"TButton\",command = OpenTxt_B )#Opens location where voice file is being stored\n",
    "TxtButton.pack(padx=50,side=LEFT)\n",
    "\n",
    "PathLabel = ttk.Label(TAB3,text = \"Listern To Speech: \",style=\"TLabel\")\n",
    "PathLabel.pack(pady=10)\n",
    "SpeechButton = ttk.Button(TAB3,style=\"TButton\",text=\"Play\",command = play )#Opens location where voice file is being stored\n",
    "SpeechButton.pack(padx=100,side=LEFT)\n",
    "StopButton = ttk.Button(TAB3,text=\"Stop\",style=\"TButton\",command = stop )#Opens location where voice file is being stored\n",
    "StopButton.pack( side=LEFT)\n",
    "\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install -c conda-forge pygobject \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "print (timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
